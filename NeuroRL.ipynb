{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neuro RLs** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TomGeorge1234/NeuroRLTutorial/blob/main/NeuroRL.ipynb)\n",
    "### **University of Amsterdam Neuro-AI Summer School ,2024**\n",
    "#### made by: **Tom George (UCL) and Jesse Geerts (Imperial)**\n",
    "\n",
    "In this tutorial we'll study and build reinforcement learning models inspired by the brain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Contents** \n",
    "0. [Import dependencies and data](#dependencies)\n",
    "1. [Rescorla-Wagner Model](#rescorla)\n",
    "2. [Temporal Difference Learning](#td)\n",
    "3. [Q-Learning](#q)\n",
    "    1. [Navigating in a grid world](#grid)\n",
    "4. [Deep Q-Learning](#dqn)\n",
    "    1. [Neuroscience inspired basis functions](#basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **0. Import dependencies and data** <a name=\"dependencies\"></a>\n",
    "Run the following code: It'll install some dependencies, download some files and import some functions. You can mostly ignore it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see code {display-mode: \"form\" }\n",
    "!pip install wget ratinabox \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm \n",
    "import os\n",
    "import wget \n",
    "from IPython.display import HTML\n",
    "#if running on colab we need to download the data and utils files\n",
    "if os.path.exists(\"NeuroRL_utils.py\"):\n",
    "    print(\"utils located\")\n",
    "    pass\n",
    "else: \n",
    "    wget.download(\"https://github.com/TomGeorge1234/NeuroRLTutorial/raw/main/NeuroRL_utils.py\")\n",
    "    print(\"...utils downloaded!\")\n",
    "\n",
    "from NeuroRL_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **1. Rescorla-Wagner** <a name=\"rescorla\"></a>\n",
    "\n",
    "Classical conditioning is where a neutral stimulus -- also called the _unconditioned stimulus, US_ -- is paired with a response-producing stimulus -- _the conditioned stimulus, CS_. After the association is learned, the neutral stimulus *alone* can produce the response.\n",
    "\n",
    "The most famous example is Pavlov's dogs: Pavlov rang a bell before feeding his dogs which would cause them to salivate. After a while, the dogs would start salivating when they heard the bell, even if no food was presented.\n",
    "\n",
    "In 1972 Rescorla and Wagner proposed a simple model to explain this learning process. The model is based on the idea that the strength of the association between the CS and US is proportional to the discrepancy between the expected and actual US.\n",
    "\n",
    "### **1.1. Model (maths)**\n",
    "Following on from the Pavlov's dogs example, suppose the bell ðŸ”” is the conditioned stimulus and the food ðŸ¦´ is the unconditioned stimulus with a response (reward) of strength $R$. The _value_ of the unconditioned stimulus, $\\hat{V}$, is the strength of the association between the  stimulus and the unconditioned response, $R$. In other words, $\\hat{V}$ is a _prediction_ of how much reward I think I'll get when I see the unconditioned stimulus. \n",
    "\n",
    "Mathematically, the model is defined as following simple equation:\n",
    "\n",
    "$$ \\hat{V} \\leftarrow \\hat{V} + \\alpha \\cdot \\underbrace{(R - \\hat{V})}_{\\delta = \\textrm{``error\"}}$$\n",
    "\n",
    "I.e. the increment in the value of the stimulus is proportional to the discrepancy between the reward (the unconditioned response) that was recieved, $R$, and the reward that was predicted from the stimulus, $\\hat{V}$. The proportionality constant $\\alpha$ is the learning rate.\n",
    "\n",
    "> ðŸ“ **Exercise 1.1** \n",
    "> 1. Consider a simple example where there is only one stimulus with zero initial value. A constant reward, $R$ is given each trial. Show the value of the stimulus after the first trial is given by $V(1) = \\alpha  \\cdot R$.\n",
    "> 2. Show that $\\hat{V}(t) = R \\cdot (1 - e^{-\\alpha\\cdot t})$  (_Hint: consider using the change of of variables $U(t) = R - \\hat{V}(t)$._)\n",
    "\n",
    "### **1.2 Model implementation (python)**\n",
    "\n",
    "Below we provide some basic code implementing a Rescorla Wagner model. \n",
    "\n",
    "Suppose we initialse a Rescorla-Wagner model with a learning rate of 0.1 and a value of 0 as `rescorlawagner = RescorlaWagner(0.1, 0)`. Some initialsation logic and plotting functions are hidden away in the `BaseRescorlaWagner` class in `NeuroRL_utils.py`. What's important is the following: \n",
    "\n",
    "**Attributes**\n",
    "- `rescorlawagner.alpha`: the learning rate\n",
    "- `rescorlawagner.V`: the currnet value of the stimulus\n",
    "- `rescorlawagner.V_history`: a list of the value of the stimulus at each trial\n",
    "- `rescorlawagner.R_history`: a list of the reward recieved at each trial\n",
    "\n",
    "**Methods**\n",
    "- `rescorlawagner.learn(R)`: updates the value of the stimulus based on the reward recieved <span style=\"color:red\"> _[TO DO: NOT YET DEFINED]_ </span>\n",
    "- `rescorlawagner.plot()`: plots the value of the stimulus over time\n",
    "\n",
    "\n",
    "> ðŸ“ **Exercise 1.2:** \n",
    "> \n",
    "> Complete the `def learn(self, R):` function to implement the Rescorla-Wagner learning rule. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagner(BaseRescorlaWagner):\n",
    "    def __init__(self, \n",
    "                 alpha=0.1, \n",
    "                 initial_V=0): \n",
    "        self.V = initial_V\n",
    "        super().__init__(n_stimuli=1, alpha=alpha)\n",
    "\n",
    "    def learn(self, R):\n",
    "        raise NotImplementedError(\"You need to implement this method\")\n",
    "        # error = ???\n",
    "        # self.V += ???\n",
    "        # self.R_history.append(R) # include these lines to store the reward and value history\n",
    "        # self.V_history.append(V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def learn(self, R):\n",
    "    error = R - self.V\n",
    "    self.V += self.alpha * error\n",
    "    self.R_history.append(R)\n",
    "    self.V_history.append(self.V) \n",
    "RescorlaWagner.learn = learn # set the learn method to the function we just defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run an experiment where a reward of 1 is given each trial. We'll plot the value of the stimulus over time using the pre-written `RescorlaWagner.plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your learning rate and reward\n",
    "alpha = 0.1\n",
    "R = 1\n",
    "\n",
    "# Create the model\n",
    "RW = RescorlaWagner(alpha=alpha)\n",
    "\n",
    "# Run the model\n",
    "for trial in range(100):\n",
    "    RW.learn(R=R)\n",
    "\n",
    "# Plot the results\n",
    "ax = RW.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Exercise 1.3**\n",
    ">\n",
    "> Plot the theoretical solution you derived earlier onto the `ax` and see if it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "t_range = np.arange(100)\n",
    "V = R * (1 - np.exp(-t_range*alpha))\n",
    "ax.plot(t_range, V, label='Analytic solution', linewidth=0.5, color='k', linestyle='--')\n",
    "ax.legend()\n",
    "ax.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Exercise 1.4**\n",
    "> \n",
    "> 1. **Acquisition:** Repeat the above experiment with a lower and a higher learning rate. What do you observe?\n",
    "> 2. **Extinction:** Repeat the above but this time reward is given only for the first 50 trials, then the reward is set to zero. What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for low learning rate acquisition goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Set your learning rate and reward\n",
    "alpha = 0.3\n",
    "R = 1\n",
    "\n",
    "# Create the model\n",
    "RW = RescorlaWagner(alpha=alpha)\n",
    "\n",
    "# Run the model\n",
    "for trial in range(100):\n",
    "    RW.learn(R=R)\n",
    "\n",
    "# Plot the results\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Higher learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for high learning rate acquisition goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Set your learning rate and reward\n",
    "alpha = 0.05\n",
    "R = 1\n",
    "\n",
    "# Create the model\n",
    "RW = RescorlaWagner(alpha=alpha)\n",
    "\n",
    "# Run the model\n",
    "for trial in range(100):\n",
    "    RW.learn(R=R)\n",
    "\n",
    "# Plot the results\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Lower learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for extinction goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Set your learning rate and reward\n",
    "alpha = 0.1\n",
    "\n",
    "# Create the model\n",
    "RW = RescorlaWagner(alpha=alpha)\n",
    "\n",
    "# Run the model\n",
    "for trial in range(50):\n",
    "    RW.learn(R=1)\n",
    "for trial in range(50): #remove the reward\n",
    "    RW.learn(R=0)\n",
    "\n",
    "# Plot the results\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Extinction experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3. Rescorla-Wagner with multiple unconditioned stimuli**\n",
    "\n",
    "It's easy to extend the Rescorla-Wagner model to multiple stimuli:\n",
    "\n",
    "* Multiple stimuli are represented by a vectors, e.g.:  \n",
    "    * Stimulus A: $\\mathbf{s} = [1, 0]$ means there are 2 stimuli, only the first of which was present on this particular trial. \n",
    "    * Stimulus B: $\\mathbf{s} = [0, 1]$ means only the second stimulus was present.\n",
    "    * Stimulus A & B: $\\mathbf{s} = [1, 1]$ means both stimuli were present.\n",
    "* A vector of association \"weights\", $\\mathbf{w}$, denotes the strength of the association between each stimulus and the unconditioned response (i.e. the value of each stimulus). \n",
    "    * $\\mathbf{w} = [w_1, w_2]$.\n",
    "* The total value of the stimuli is the sum of the values of each stimulus present on a given trial: \n",
    "$$ \\hat{V} = \\mathbf{s} \\cdot \\mathbf{w} $$\n",
    "\n",
    "The full Rescorla-Wagner model is then:\n",
    "$$\\mathbf{w} = \\mathbf{w} + \\alpha \\big(R - \\hat{V}\\big) \\cdot \\mathbf{s}$$\n",
    "\n",
    "> ðŸ“ **Exercise 1.5**\n",
    ">\n",
    "> Reason why $\\mathbf{s}$ now appears in the learning rule.\n",
    "\n",
    "> ðŸ“ **Exercise 1.6**\n",
    "> \n",
    "> As before, complete the `def learn(self, R, S):` function to implement the Rescorla-Wagner learning rule for multiple stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagner_multistim(BaseRescorlaWagner):\n",
    "    def __init__(self, n_stimuli=2, alpha=0.1, initial_value=0): \n",
    "        self.W = np.zeros(n_stimuli)\n",
    "        super().__init__(n_stimuli=n_stimuli, alpha=alpha)\n",
    "\n",
    "    def learn(self, S, R):\n",
    "        raise NotImplementedError(\"You need to implement this method\")\n",
    "        # V = ????  # calculate the value of the stimuli\n",
    "        # error = # ???? # calculate the error\n",
    "        # self.W += # ???? # update the weights\n",
    "\n",
    "        # store the history\n",
    "        # self.W_history = np.vstack([self.W_history, self.W])\n",
    "        # self.R_history.append(R)\n",
    "        # self.V_history.append(S @ self.W)\n",
    "        # self.S_history.append(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def learn(self, S, R):\n",
    "    V = S @ self.W # calculate the value of the stimuli\n",
    "    error = R - V # calculate the error\n",
    "    self.W += alpha * S * error # update the weights\n",
    "\n",
    "    # store the history\n",
    "    self.W_history.append(self.W.copy())\n",
    "    self.R_history.append(R)\n",
    "    self.V_history.append(S @ self.W)\n",
    "    self.S_history.append(S)\n",
    "\n",
    "# Set the learn method to the function we just defined.\n",
    "RescorlaWagner_multistim.learn = learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.7**:\n",
    "1. With your partner implement these four experiments in the Rescorla-Wagner model:\n",
    "    1. **Blocking**\n",
    "        * A single stimulus is paired with the US (A --> R), then a compound stimulus is paired with the US (AB --> R). What happens?\n",
    "    2. **Overshadowing**\n",
    "        * Two stimuli are paired with a reward (AB --> R) but one is much more salient than the other, e.g. $\\vec{S} = [1, 0.1]$.\n",
    "    3. **Overexpectation**\n",
    "        * Two stimuli are seperately paired with the US (A --> R, B --> R), then the compound stimulus is presented (AB --> ?). What happens?\n",
    "    4. **Conditioned Inhibition**\n",
    "        * A single stimulus is paired with the US (A --> R) then a second stimulus is added and the reward is removed. (AB --> _). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the blocking experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_multistim(n_stimuli=2, alpha=0.1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1,0]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1,1]), R=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this plot: \n",
    "\n",
    "* The **top** plot shows the stimuli which were present on each trial. Their transparency denotes their strength. \n",
    "* The **middle** plot shows the association weights of the stimuli.\n",
    "* The **bottom** plot shows the total value of the presented stimuli $V = \\mathbf{s}\\cdot\\mathbf{w}$ along with the reward recieved on each trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the overshadowing experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(100):\n",
    "    RW.learn(S=np.array([0.9, 0.1]), R=1)\n",
    "ax = RW.plot()\n",
    "ax[0].set_title(\"Overshadowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the overexpectation experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([0, 1]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 1]), R=1)\n",
    "ax = RW.plot()\n",
    "ax[0].set_title(\"Overexpectation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the inhibition experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 1]), R=0)\n",
    "ax = RW.plot()\n",
    "ax[0].set_title(\"Inhibition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Exercise 1.8**\n",
    ">\n",
    "> 1. Starting from the inhibition experiment above, extend it so that the reward prediction goes negative.\n",
    "> 2. Simulate a conditioning experiment with three stimuli, where the first two are paired with the US and the third is not. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code for the negative reward prediction experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 1]), R=0)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([0, 1]), R=1)\n",
    "ax = RW.plot()\n",
    "ax[0].set_title(\"Negative reward prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Temporal difference learning** <a name=\"td\"></a>\n",
    "\n",
    "One limitation of the Rescorla-Wagner model is that it doesn't take into account the temporal structure of the environment. Associations are made between stimuli _now_ and rewards _now_. In reality, rewards are often delayed.\n",
    "\n",
    "Temporal difference (TD) learning takes into account the temporal structure of the environment. As you learnt in today's lecture the core idea is thatthe \"value\" of a state is based not only on the reward recieved _now_ but on all the rewards that _will be_ recieved in the future.\n",
    "\n",
    "$$V(S_t) = \\mathbb{E} \\big[ \\underbrace{R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots}_{G_t = \\textrm{``return'' from state $_{S_t}$}}\\big]$$\n",
    "\n",
    "To formalise notation, we'll define $\\hat{V}(S_t)$ as _our current estimate of the value of a state_ $V(S_t)$. The goal of learning is to to refine $\\hat{V}(S_t)$ to be as close as possible to $V(S_t)$.\n",
    "\n",
    "> ðŸ“ **Exercise 2.1**\n",
    ">\n",
    "> By considering the following loss function: \n",
    "> $$L_t = \\big[ \\hat{V}(S_t) - V(S_t) \\big]^2$$\n",
    "> show, by gradient descent in $\\hat{V}(S_{t})$, that the optimal update rule for $\\hat{V}(S_t)$ is:\n",
    "> $$\\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha \\big[ V(S_t) - \\hat{V}(S_t) \\big]$$\n",
    "\n",
    "### **2.1 Monte-Carlo learning**\n",
    "One way to estimate the value of a state is to wait until the end of each episode, collecting all the rewards that were recieved along the way and calculate the single-episode return $G_t$. Since the _expectation_ of $G_t$ is equal to $V(S_t)$ this is equivalent to _stochastic gradient descent_ of the loss function and is called Monte-Carlo learning.\n",
    "\n",
    "$$ \\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha  \\big[ G_t - \\hat{V}(S_t) \\big] $$\n",
    "\n",
    "Although in theory this does work, in practice, Monte-Carlo learning is often infeasible because it requires waiting until the end of each episode to update the value of each state. This turns out to be quite a serious limitation in real-world applications - imagine waiting until the end of a game of chess to update the value of each state! Or worse, some environments don't have a clearly defined end such as the game of life we're all currently playing. \n",
    "\n",
    "This is where TD learning comes in...\n",
    "\n",
    "\n",
    "### **2.2 TD-Learning**\n",
    "The key idea behind TD-learning is to estimate the value of a state by bootstrapping from the value of the next state. \n",
    "\n",
    "> ðŸ“ **Exercise 2.2**\n",
    ">\n",
    "> 1. Show that the value of a state can be written as the sum of the reward recieved at that state and the value of the next state. i.e.\n",
    "> $$V(S_t) = \\mathbb{E} [R_t + \\gamma V(S_{t+1})]. $$\n",
    "\n",
    "This is called the _Bellman equation_ and is the basis of TD-learning. Its encodes a _very_ important idea: \n",
    "\n",
    "**Bellman Equation:  The future value of a state now is equal to the reward recieved now plus the value of the next state (discounted a little bit).**\n",
    "\n",
    "But wait! How do we know the value of the next state? We don't! That's why we're learning! So we'll use our current estimate of the value of the next state, $\\hat{V}(S_{t+1})$ as a _proxy_.\n",
    "\n",
    "$$V(S_t) = \\mathbb{E} [\\color{red}{\\underbrace{R_t + \\gamma V(S_{t+1})}_{\\textrm{I don't know this}}} \\color{d}{}] \\approx  \\mathbb{E}[\\color{green}{\\underbrace{R_t + \\gamma \\hat{V}(S_{t+1})}_{\\textrm{I do know this}}}\\color{d}{}] $$\n",
    "\n",
    "This gives us the TD-learning update rule:\n",
    "\n",
    "$$\\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha \\big[\\underbrace{R_t + \\gamma \\hat{V}(S_{t+1}) - \\hat{V}(S_t)}_{\\delta_t = \\textrm{``TD-error''}} \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Exercise 2.3**\n",
    ">\n",
    "> 1. Implement the TD-learning update rule in the `def learn(self, R, S, S_next, alpha):` function in the `TDLearner` class below.\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLearner(BaseTDLearner):\n",
    "    def __init__(self, gamma=0.5, alpha=0.1, n_states=10):\n",
    "        super().__init__(gamma=gamma, alpha=alpha, n_states=n_states)\n",
    "\n",
    "    def perform_episode(self, \n",
    "                        states : np.ndarray, \n",
    "                        rewards : np.ndarray,):\n",
    "\n",
    "        T_episode = len(states) + 1 # get the length of the episode (including the initial None state)\n",
    "\n",
    "        # Insert an unrewarded \"None\" state at the beginning to indicate the start of an episode\n",
    "        rewards = np.insert(rewards, 0, 0)\n",
    "        states = np.insert(states.astype(object), [0, len(states)], [None, None])\n",
    "\n",
    "        # Loop over the states and learn from each transition\n",
    "        TD_errors = np.empty(T_episode) # store the TD errors\n",
    "        for i in range(T_episode):\n",
    "            # Learn from this transition\n",
    "            TD_errors[i] = self.learn(states[i], states[i+1], rewards[i])\n",
    "        \n",
    "        # Save to history \n",
    "        self.V_history.append(self.V.copy())\n",
    "        self.S_history.append(states[:T_episode])\n",
    "        self.TD_history.append(TD_errors)\n",
    "        self.R_history.append(rewards)\n",
    "        \n",
    "    def learn(self, S, S_next, R):\n",
    "        raise NotImplementedError(\"You need to implement this method\")\n",
    "        # Get's the value of the current and next state\n",
    "        # V = self.V[S] if S is not None else 0\n",
    "        # V_next = self.V[S_next] if S_next is not None else 0\n",
    "        \n",
    "        # Calculates the TD error (hint remember to use self.gamma\n",
    "        # TD_error = # ???\n",
    "\n",
    "        # Updates the value of the current state\n",
    "        # if S is not None:\n",
    "            # self.V[S] = # ???\n",
    "\n",
    "        #  return TD_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def learn(self, S, S_next, R):\n",
    "    # Get's the value of the current and next state\n",
    "    V = self.V[S] if S is not None else 0\n",
    "    V_next = self.V[S_next] if S_next is not None else 0\n",
    "    \n",
    "    # Calculates the TD error \n",
    "    TD_error = R + self.gamma * V_next - V\n",
    "\n",
    "    # Updates the value of the current state\n",
    "    if S is not None:\n",
    "        self.V[S] = self.V[S] + self.alpha * TD_error\n",
    "\n",
    "    return TD_error\n",
    "\n",
    "TDLearner.learn = learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Training a TD-Learner on a sequence of states** \n",
    "\n",
    "Now we're going to train the TD learner in a simple environment. \n",
    "\n",
    "States are represented by integers, e.g. $S_t = 0$ means the agent is in state 0. And states are connected in a sequence, e.g. $S_t = 0 \\rightarrow S_{t+1} = 1 \\rightarrow S_{t+2} = 2 \\ldots$.\n",
    "\n",
    "At the final state, the agent recieves a reward of $R = 1$ and the episode ends.\n",
    "\n",
    "To run this episode we'll use the `perform_episode` function which takes a list of states and list of rewards and loops through them, saving the results as it goes: \n",
    "\n",
    "```python\n",
    "TD.perform_episode(\n",
    "    states  = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
    "    rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters \n",
    "n_episodes = 40\n",
    "gamma = 0.95\n",
    "alpha = 0.8 # for now use a high learning rate\n",
    "n_states = 10\n",
    "\n",
    "# Initialize the TD learner\n",
    "TD = TDLearner(gamma=gamma, n_states=n_states, alpha=alpha)\n",
    "\n",
    "# Run the experiment\n",
    "for episode in range(n_episodes):\n",
    "    TD.perform_episode(\n",
    "        states=np.arange(n_states), \n",
    "        rewards=np.eye(n_states)[n_states-1],  ) \n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    TD.perform_episode(\n",
    "        states=np.arange(n_states), \n",
    "        rewards=np.zeros(n_states),  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = TD.animate_plot()\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Exercise 2.4**\n",
    "> \n",
    "> With your partner, answer the following questions:\n",
    "> 1. After learning has converged, what is the value of state 0? \n",
    "> 2. Approximately how fast does the value bump move backwards? How does this relate to the notion of one-step bootstrapping. \n",
    "> 3. Why does a residual TD-error accumulate at the start? Understanding this is important for understanding TD-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <button onclick=\"this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none';\">\n",
    "    Click to reveal\n",
    "  </button>\n",
    "  <div style=\"display: none;\">\n",
    "    <h1></h1>\n",
    "    <p>Exercise 2.4 solutions</p>\n",
    "    <ul>\n",
    "      <li>State 9 has value $V = R$, state 8 has value $V = 0 + \\gamma * R = \\gamma R$, state 7 has value $V = 0 + \\gamma^2 R$, etc. So state 0 has value $V = \\gamma^9 R$. So if $R=1$ and $\\gamma = 0.9$ then $V = 0.9^9 = 0.387420489$.</li>\n",
    "      <li>Suppose $\\alpha = 1$ and $\\gamma$ is close to one. The first time the agent recieves the reward at state 9 it's value will be updated to $V = 1$ and no further learning will occur on this state (its TD error will be zero). On the next trial the value of state 8 will be updated due to the a TD error because the new value of upcoming stae 9 wasn't predicted. Thus, the bump moves backwards at approximately a rate of **one-step-each-trial**.</li>\n",
    "      <li>The residual TD-error at the start is because the first state is never predictable. Pavlov's dog may be able to associate the bell with the food, but it can't predict the bell so hearing the bell will always come as a positive surprise (aka. a positive TD-error).</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Exercise 2.5**\n",
    ">\n",
    "> Try the following experiments and discuss the results: \n",
    "> 1. Decrease the learning rate.\n",
    "> 2. Increase the discount factor.\n",
    "> 3. Increase the number of states in the sequence.\n",
    "> 4. Add noise to the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Exercise 2.6**\n",
    ">\n",
    "> 1. What would happen to the TD error if a small positive reward is given at the end (as we just simulated) and then, after learning has converged, the reward is removed? _Hint: think about what happens in the brain, and why_.\n",
    "> 2. Simulate this experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **3. Q-Values and Policy Improvement** <a name=\"q\"></a>\n",
    "\n",
    "So far we have only considered environments where there is no choice. The agent is simply moving through a sequence of states. In the real world agents have choices (called _actions_) and the value of a state is not just the value of the state itself but the value of the state _and the action taken in that state_.\n",
    "\n",
    "We need to introduce the formal concept of a _policy_ which is a function that maps states to actions. The value of a state under a policy is the expected return (i.e. discounted sum of all future rewards) from that state under that policy. Policies are often denoted by $\\pi$, they are functions which map states to actions\n",
    "\n",
    "$$ \\pi : S \\rightarrow A $$\n",
    "\n",
    "In an environment with a policy, states, actions and rewards progress as follows:\n",
    "\n",
    "$$ S_0 \\xrightarrow{A_0} R_1, S_1 \\xrightarrow{A_1} R_2, S_2 \\xrightarrow{A_2} R_3, S_3 \\xrightarrow{A_3} R_4, S_4 \\xrightarrow{A_4} \\ldots $$\n",
    "\n",
    "The _action_ the agent took at each state determines the state the agent ends up in next which may (or may not) come with a reward.\n",
    "\n",
    "### **3.1 Q-Values**\n",
    "\n",
    "The \"value'' of a state now depends not just on the state but on the action taken in that state and the policy that was followed thereafter. \n",
    "\n",
    "Q-values represent a natural generalisation of the state value of a state-action pair under a policy: \n",
    "\n",
    "$$ Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} \\big[ R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots \\big| S_t = s, A_t = a \\big] $$\n",
    "\n",
    "In plain English this say: \"$Q_{\\pi}(s, a)$ is the expected return from taking action $a$ in state $s$ and then following policy $\\pi$.\". Note the expectation is over $\\pi$ because the policy determines the future actions, states and rewards. The policy (which actions are taken in which states) and the environment (which states and rewards are reached from which states) could both be stochastic.\n",
    "\n",
    "The equivalent learning rule to the TD-learning rule for Q-values is:\n",
    "\n",
    "$$\\hat{Q}_{\\pi}(S_t, A_t) \\leftarrow \\hat{Q}_{\\pi}(S_t, A_t) + \\alpha \\big[ R_t + \\gamma \\hat{Q}_{\\pi}(S_{t+1}, A_{t+1}) - \\hat{Q}_{\\pi}(S_t, A_t) \\big]$$\n",
    "\n",
    "This is often called the SARSA learning rule because it takes into account the **S**tate, the **A**ction, the **R**eward, the next **S**tate and the next **A**ction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adapt the TD-learner to learn Q-values. It's pretty simple, we just need: \n",
    "\n",
    "1. Change `self.V` (a list) to `self.Q` (an array) to store Q-values for each state-action pair. \n",
    "    - `self.Q[s, a]` is the Q-value of state `s` and action `a`.\n",
    "2. Change the `learn` function to update Q-values instead of state values. Instead of \n",
    "    - `self.learn(S, S_next, R)` updating `self.V[S]`...\n",
    "    - `self.learn(S, S_next, A, A_next, R)` should update `self.Q[S, A]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDQLearner(BaseTDLearner):\n",
    "    def __init__(self, gamma=0.5, alpha=0.1, n_states=10, n_actions=2):\n",
    "\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        self.Q_history = []\n",
    "        self.A_history = []\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        super().__init__(gamma=gamma, alpha=alpha, n_states=n_states)\n",
    "\n",
    "    def perform_episode(self, \n",
    "                        states : np.ndarray, \n",
    "                        actions : np.ndarray,\n",
    "                        rewards : np.ndarray,):\n",
    "\n",
    "        T_episode = len(states) + 1 # get the length of the episode (including the initial None state)\n",
    "\n",
    "        # Insert an unrewarded \"None\" state at the beginning to indicate the start of an episode\n",
    "        rewards = np.insert(rewards, 0, 0)\n",
    "        states = np.insert(states.astype(object), [0, len(states)], [None, None])\n",
    "        actions = np.insert(actions.astype(object), [0, len(actions)], [None, None])\n",
    "\n",
    "        # Loop over the states and learn from each transition\n",
    "        TD_errors = np.empty(T_episode) # store the TD errors\n",
    "        for i in range(T_episode):\n",
    "            # Learn from this transition\n",
    "            TD_errors[i] = self.learn(states[i], states[i+1], actions[i], actions[i+1], rewards[i])\n",
    "        \n",
    "        # Save to history \n",
    "        self.Q_history.append(self.Q.copy())\n",
    "        self.S_history.append(states[:T_episode])\n",
    "        self.A_history.append(states[:T_episode])\n",
    "        self.TD_history.append(TD_errors)\n",
    "        self.R_history.append(rewards)\n",
    "        self.V_history.append(np.mean(self.Q, axis=1))\n",
    "        \n",
    "    def learn(self, S, S_next, A, A_next, R):\n",
    "        # Get's the value of the current and next state\n",
    "        Q = self.Q[S,A] if S is not None else 0\n",
    "        Q_next = self.Q[S_next, A_next] if S_next is not None else 0\n",
    "        \n",
    "        # Calculates the TD error (hint remember to use self.gamma\n",
    "        TD_error = R + self.gamma * Q_next - Q\n",
    "\n",
    "        # Updates the value of the current state\n",
    "        if S is not None:\n",
    "            self.Q[S,A] = self.Q[S,A] + self.alpha * TD_error \n",
    "\n",
    "        return TD_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Exercise 3.1**\n",
    "> \n",
    "> Perform the following experiment: the agent progresses through a series of states _either_:\n",
    "> 1. Moving to the right (action $A = 0$), recieving a  reward ($R=1$) at the end of the episode (where state $S=9$).\n",
    ">    - `states   = np.array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])`\n",
    ">    - `actions = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])`\n",
    ">    - `rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])`\n",
    "> 2. Moving to the left (action $A = 1$), recieving a no reward ($R=0$) at the end of the episode (where state $S=0$).\n",
    ">    - `states   = np.array([ 9, 8, 7, 6, 5, 4, 3, 2, 1, 0])`\n",
    ">    - `actions = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])`\n",
    ">    - `rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])`\n",
    "\n",
    "> ðŸ“ **Exercise 3.2**\n",
    ">\n",
    "> Using `TDQLearner.Q` plot the Q-values of each state-action pair. What difference do you observe between the Q-values of _the same two states_ under the two different actions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.90 # discount factor\n",
    "alpha = 0.5 # learning rate\n",
    "n_episodes = 100\n",
    "\n",
    "# Make the TDQ learner\n",
    "TDQ = TDQLearner(gamma=gamma, alpha=alpha, n_states=10, n_actions=2)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    \n",
    "    # randomly choose between a left or right episode\n",
    "    episode_type = np.random.choice(['left', 'right'])\n",
    "\n",
    "    # write code which generates the states, actions and rewards for the episode and then performs the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "gamma = 0.90 # discount factor\n",
    "alpha = 0.5 # learning rate\n",
    "n_episodes = 100\n",
    "\n",
    "# Make the TDQ learner\n",
    "TDQ = TDQLearner(gamma=gamma, alpha=alpha, n_states=10, n_actions=2)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    \n",
    "    # randomly choose between a left or right episode\n",
    "    episode_type = np.random.choice(['left', 'right'])\n",
    "\n",
    "    if episode_type == 'left':\n",
    "        states = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        actions = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "    elif episode_type == 'right':\n",
    "        states = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n",
    "        actions = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "        rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, -1])\n",
    "    \n",
    "    TDQ.perform_episode(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to plot the Q values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title Click to see solution {display-mode: \"form\" }\n",
    "Q_values = TDQ.Q\n",
    "fig, ax = plt.subplots(1,1 , figsize=(4,2))\n",
    "ax.bar(np.arange(10), Q_values[:,0], color='C0', label='Action = 0 (\"Right\")', alpha=0.5)\n",
    "ax.bar(np.arange(10), Q_values[:,1], color='C1', label='Action = 1 (\"Left\")', alpha=0.5)\n",
    "ax.axhline(0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('State')\n",
    "ax.legend() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Policy Improvement**\n",
    "\n",
    "Finally, we can use the Q-values to improve the policy. The optimal policy is the policy that maximises the expected return from each state.\n",
    "\n",
    "$$ \\pi^*(s) = \\arg\\max_a Q^*(s, a) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array( # 0 = empty, 1 = wall, 2 = reward\n",
    "    [\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],        \n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ])\n",
    "\n",
    "minigrid = MiniGrid(\n",
    "    grid = grid,\n",
    "    init_agent_pos=(3,7),\n",
    "    reward_locations = [(10,2),]\n",
    "    )\n",
    "\n",
    "tdqlearner = TDQLearner(\n",
    "    gamma=0.95, \n",
    "    alpha=0.1, \n",
    "    n_states=minigrid.n_states, \n",
    "    n_actions=minigrid.n_actions)\n",
    "\n",
    "# minigrid.render()\n",
    "\n",
    "minigrid.plot_Q_values(tdqlearner.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_lengths = []\n",
    "for i in (pbar := tqdm(range(500))):\n",
    "    try: \n",
    "        episode_data = minigrid.perform_episode(Q_values=tdqlearner.Q, epsilon=0.1)\n",
    "        tdqlearner.perform_episode(\n",
    "            states=episode_data['states'],\n",
    "            actions=episode_data['actions'],\n",
    "            rewards=episode_data['rewards'])\n",
    "        minigrid.reset()\n",
    "        pbar.set_description(f\"Episode length (recent average): {episode_data['smoothed_episode_length'] : .0f}\")\n",
    "        episode_lengths.append(len(episode_data['states']))\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid.plot_Q_values(tdqlearner.Q)\n",
    "minigrid.plot_episode_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, figsize=(10,2))\n",
    "for i in range(5):\n",
    "    minigrid.plot_episode(i, ax=ax[i])\n",
    "fig.suptitle(\"First 5 episodes\")\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=(10,2))\n",
    "for i in range(5):\n",
    "    minigrid.plot_episode(-i-1, ax=ax[i])\n",
    "fig.suptitle(\"Last 5 episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Policy Improvement: Finding the _best_ policy**\n",
    "\n",
    "The goal of reinforcement learning is to find the _best_ policy i.e. the policy that maximises the expected return from every state!\n",
    "\n",
    "\n",
    "In fact the code learning rule is so similiar w can hijack the exact same code from the TD-learner and use it to learn Q-values, all we'll do different is _pretend_ that each state-action pair is a \"state\". We can do this by uniquely labelling each state-action pair with a single integer.\n",
    "\n",
    "$$ (x=0, y=0, action=\"North\") \\rightarrow 0, (x=0, y=0, action=\"East\") \\rightarrow 1, (x=0, y=0, action=\"South\") \\rightarrow 2, (x=0, y=0, action=\"West\") \\rightarrow 3, \\ldots $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,3) in [(1,3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurorl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
