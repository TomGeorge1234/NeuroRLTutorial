{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neuro RL** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TomGeorge1234/NeuroRLTutorial/blob/main/NeuroRL.ipynb)\n",
    "## **University of Amsterdam Neuro-AI Summer School, 2024**\n",
    "### made by: **Tom George (UCL) and Jesse Geerts (Imperial)**\n",
    "\n",
    "<img src=\"./figures/tom.png\" height=100><img src=\"./figures/jesse.png\" height=100>\n",
    "(this is what we look like...find us wandering around)\n",
    "\n",
    "In this tutorial we'll study and build reinforcement learning models inspired by the brain. By the end you'll understand, and be able to construct, a series of simple but surpringly powerful model of how agents learn to navigate in environment and find rewards. For example...\n",
    "\n",
    "<center><img src=\"./figures/rl_animation.gif\" width=500></center>\n",
    "\n",
    "_Figure 1: An agent has learn to navigate around a wall towards a hidden reward using place cell state features and a simple Q-value learning algorithm._\n",
    "\n",
    "### Exercises \n",
    "\n",
    "> üìù **Exercise X.Y:** blah blah blah...\n",
    "\n",
    "You should complete these exersizes to the best of your ability. Exercises tagged as **[ADVANCED]** are optional, intended only for those who feel comfortable with the material and want to push their understanding. \n",
    "- Coding exercises are tagged with üêç and always come with solutions hidden. To get the most out of this tutorial you must try to complete these before checking your solution. \n",
    "- Maths and/or discussion exercises are tagged with üí≠. Your TA may have given you solutions to these in a seperate notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Contents** \n",
    "0. [Import dependencies and data](#dependencies) \n",
    "1. [Rescorla-Wagner Model](#rescorla) (~60 mins)\n",
    "    1. [Rescorla-Wagner with one conditioned stimulus](#rescorla_signlestim)\n",
    "    2. [Rescorla-Wagner with multiple conditioned stimuli](#rescorla_multistim)\n",
    "2. [Temporal Difference Learning](#td) (~60 mins)\n",
    "    1. [Markov Reward Processes](#mrp)\n",
    "    2. [Monte Carlo Learning](#montecarlo)\n",
    "    3. [Temporal Difference Learning](#bootstrapping)\n",
    "3. [Q-Values and Policy Improvement](#q) (~60 mins)\n",
    "    1. [Markov Decision Processes](#mdp)\n",
    "    2. [Q-Values](#qlearning)\n",
    "    3. [SARSA](#sarsa)\n",
    "    4. [Policy Improvement](#policyimprovement)\n",
    "    5. [Policy Iteration](#policyiteration)\n",
    "4. [State features and function approximation](#dqn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **0. Import dependencies and data** <a name=\"dependencies\"></a>\n",
    "Run the following code: It'll install some dependencies, download some files and import some functions. You can mostly ignore it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see code {display-mode: \"form\" }\n",
    "!pip install wget ratinabox torch\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm \n",
    "import ratinabox\n",
    "from ratinabox import Environment, Agent, Neurons \n",
    "import os\n",
    "import wget \n",
    "from IPython.display import HTML\n",
    "#if running on colab we need to download the data and utils files\n",
    "if os.path.exists(\"NeuroRL_utils.py\"):\n",
    "    print(\"utils located\")\n",
    "    pass\n",
    "else: \n",
    "    wget.download(\"https://github.com/TomGeorge1234/NeuroRLTutorial/raw/main/NeuroRL_utils.py\")\n",
    "    print(\"...utils downloaded!\")\n",
    "\n",
    "from NeuroRL_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **1. Rescorla-Wagner** <a name=\"rescorla\"></a>\n",
    "\n",
    "Classical conditioning is where a neutral stimulus (the conditioned stimulus) is paired with a response-producing stimulus (the unconditioned stimulus). After the association is learned, the neutral stimulus *alone* can produce the response.\n",
    " \n",
    "The most famous example is Pavlov's dogs: Pavlov rang a bell before feeding his dogs which would cause them to salivate. After a while, the dogs would start salivating when they heard the bell, even if no food was presented.\n",
    "\n",
    "In 1972 Rescorla and Wagner proposed a simple model to explain this learning process. The model is based on the idea that the strength of the association between the CS and US is proportional to the discrepancy between the expected and actual US.\n",
    "\n",
    "### **1.1. Model (maths)**<a name=\"rescorla_singlestim\"></a>\n",
    "Following on from the Pavlov's dogs example, suppose the bell üîî is the conditioned stimulus, $S$, and the food ü¶¥ is the unconditioned stimulus with a response (reward) of strength $R$. The bell is paired with the food allowing an association to be learned. We notate this as follows: \n",
    "$$ S \\rightarrow R $$\n",
    "\n",
    "Under the Rescorla-Wagner model, the goal is to learn the _value_ of the unconditioned stimulus:\n",
    "\n",
    "$$ V(S) = \\mathbb{E}[R] $$ \n",
    "\n",
    "(Note: this slightly excessive notation now will come in useful later). We can \"learn\" this association by updating $\\hat{V}(S)$ (our current _estimate_ of the value of the stimulus) based on the following trivial learning rule: \n",
    "\n",
    "$$ \\hat{V} \\leftarrow \\hat{V} + \\alpha \\cdot \\underbrace{(R - \\hat{V})}_{\\delta = \\textrm{``error\"}}$$\n",
    "\n",
    "This is Rescorla-Wagner. I.e. the increment in the value of the stimulus is proportional to the discrepancy between the reward (the unconditioned response) that was received, $R$, and the reward that was predicted from the stimulus, $\\hat{V}$. The proportionality constant $\\alpha$ is the learning rate.\n",
    "\n",
    "$\\delta$ is the _prediction error_ and is the key concept in reinforcement learning. It is the discrepancy between what was expected and what was received. \n",
    "- Positive $\\delta$ means the reward was better than expected, so the value of the stimulus should be increased.\n",
    "- Negative $\\delta$ means the reward was worse than expected, so the value of the stimulus should be decreased.\n",
    "\n",
    "> üìù **Exercise 1.1** \n",
    "> 1. üí≠ Consider a simple example where there is only one stimulus with zero initial value. A constant reward, $R$ is given each trial. Show the value of the stimulus after the first trial is given by $V(1) = \\alpha  \\cdot R$.\n",
    "> 2. üí≠ **[ADVANCED]** Show that $\\hat{V}(t) = R \\cdot (1 - e^{-\\alpha\\cdot t})$  (_Hint: you'll need to reformulate the update rule as an ODE and consider using the change of of variables $\\delta(t) = R - \\hat{V}(t)$ and assume $\\alpha$ is small_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Model implementation (python)**\n",
    "\n",
    "Below we provide some basic code implementing a Rescorla Wagner model. \n",
    "\n",
    "Suppose we initialse a Rescorla-Wagner model with a learning rate of 0.1 and an initial value estimate of 0 as \n",
    "\n",
    "```python\n",
    "rescorlawagner = RescorlaWagner(0.1, 0)\n",
    "```\n",
    "\n",
    "Some initialsation logic and plotting functions are hidden away in the `BaseRescorlaWagner` class in `NeuroRL_utils.py`. What's important is the following: \n",
    "\n",
    "**Attributes**\n",
    "- `rescorlawagner.alpha`: is the learning rate, $\\alpha$\n",
    "- `rescorlawagner.V`: the current value estimate of the stimulus, $\\hat{V}(t)$\n",
    "- `rescorlawagner.V_history`: a list of the value of the stimulus at each trial, [$\\hat{V}(0)$, $\\hat{V}(1)$, ...]\n",
    "- `rescorlawagner.R_history`: a list of the reward received at each trial, [0, $R(1)$, $R(2)$, ...] (the reward at trial 0 is always set to zero)\n",
    "\n",
    "**Methods**\n",
    "- `rescorlawagner.learn(R)`: updates the value of the stimulus based on the reward received <span style=\"color:red\"> _[TO BE WRITTEN BY YOU]_ </span>\n",
    "- `rescorlawagner.plot()`: plots the value of the stimulus over time\n",
    "\n",
    "\n",
    "> üìù **Exercise 1.2:** \n",
    "> \n",
    "> 1. üêç Complete the `def learn(self, R):` function to implement the Rescorla-Wagner learning rule. Write the lines marked `????`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagner(BaseRescorlaWagner):\n",
    "    def __init__(self, \n",
    "                 alpha=0.1, \n",
    "                 initial_V=0): \n",
    "        self.V = initial_V\n",
    "        super().__init__(n_stimuli=1, alpha=alpha)\n",
    "\n",
    "    def learn(self, R):\n",
    "        raise NotImplementedError(\"You need to implement this method\")\n",
    "        # error = ????\n",
    "        # self.V += ????\n",
    "        # self.R_history.append(R) # include these lines to store the reward and value history\n",
    "        # self.V_history.append(V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def learn(self, R):\n",
    "    error = R - self.V\n",
    "    self.V += self.alpha * error\n",
    "    self.R_history.append(R)\n",
    "    self.V_history.append(self.V) \n",
    "RescorlaWagner.learn = learn # set the learn method to the function we just defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run an experiment where a reward of 1 is given each trial. We'll plot the value of the stimulus over time using the pre-written `RescorlaWagner.plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your learning rate and reward\n",
    "alpha = 0.1\n",
    "R = 1\n",
    "\n",
    "# Create the model\n",
    "rescorlawagner = RescorlaWagner(alpha=alpha)\n",
    "\n",
    "# Run the model\n",
    "for trial in range(100):\n",
    "    rescorlawagner.learn(R=R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 1.3**\n",
    ">\n",
    "> Print the past value of the stimulus at each trial and check it approaches 1 (the reward value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "print(rescorlawagner.V_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 1.4**\n",
    ">\n",
    "> 1. üêç Plot the value history using the `ax = rescorlawagner.plot()` method. \n",
    "> 2. üêç On top of this, plot the theoretical solution you derived earlier (`ax.plot( ... )`) and check if it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Plot the results\n",
    "ax = rescorlawagner.plot()\n",
    "plt.close()\n",
    "\n",
    "# Plot the analytic solution\n",
    "t_range = np.arange(100)\n",
    "V = R * (1 - np.exp(-t_range*alpha))\n",
    "ax.plot(t_range, V, label='Analytic solution', linewidth=0.5, color='k', linestyle='--')\n",
    "ax.legend()\n",
    "ax.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 1.5**\n",
    "> \n",
    "> 1. üêç **Acquisition:** Repeat the above experiment with a lower and a higher learning rate. What do you observe?\n",
    "> 2. üí≠ TO THINK What happens if the learning rate $\\alpha = 1$. Why might this not be a good idea all the time? \n",
    "> 3. üêç **Extinction:** Repeat the above but this time reward is given only for the first 50 trials, then the reward is set to zero. What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for low learning rate acquisition goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Set your learning rate and reward\n",
    "alpha = 0.5\n",
    "R = 1\n",
    "\n",
    "# Create the model\n",
    "RW = RescorlaWagner(alpha=alpha)\n",
    "\n",
    "# Run the model\n",
    "for trial in range(100):\n",
    "    RW.learn(R=R)\n",
    "\n",
    "# Plot the results\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Higher learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for high learning rate acquisition goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Set your learning rate and reward\n",
    "alpha = 0.05\n",
    "R = 1\n",
    "\n",
    "# Create the model\n",
    "RW = RescorlaWagner(alpha=alpha)\n",
    "\n",
    "# Run the model\n",
    "for trial in range(100):\n",
    "    RW.learn(R=R)\n",
    "\n",
    "# Plot the results\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Lower learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for extinction goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Set your learning rate and reward\n",
    "alpha = 0.1\n",
    "\n",
    "# Create the model\n",
    "RW = RescorlaWagner(alpha=alpha)\n",
    "\n",
    "# Run the model\n",
    "for trial in range(50):\n",
    "    RW.learn(R=1)\n",
    "for trial in range(50): #remove the reward\n",
    "    RW.learn(R=0)\n",
    "\n",
    "# Plot the results\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Extinction experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3. Rescorla-Wagner with multiple  stimuli**<a name=\"rescorla_multistim\"></a>\n",
    "\n",
    "It's easy to extend the Rescorla-Wagner model to multiple stimuli:\n",
    "\n",
    "* Stimuli are now represented by vectors. If there are two possible stimuli we can use a 2D vector e.g.:  \n",
    "    * Just stimulus A: $\\mathbf{s} =[1, 0]$ \n",
    "    * Just stimulus B: $\\mathbf{s} =[0, 1]$\n",
    "    * Stimulus A & B: $\\mathbf{s} =[1, 1]$\n",
    "    * Stimulus A weakly and B strongly: $\\mathbf{s} =[0.1, 1.0]$\n",
    "    * ...etc. \n",
    "\n",
    "* A vector of association \"weights\", $\\mathbf{w}$, denotes the strength of the association between each stimulus and the unconditioned response (i.e. the value of each stimulus). \n",
    "    * $\\mathbf{w} = [w_1, w_2]$.\n",
    "* The total value of the stimuli is the sum of the values of each stimulus present on a given trial: \n",
    "$$ \\hat{V}(\\mathbf{s}) = \\mathbf{s} \\cdot \\mathbf{w}  = s_1 \\cdot w_1 + s_2 \\cdot w_2$$\n",
    "\n",
    "The full Rescorla-Wagner model is then:\n",
    "$$\\mathbf{w} = \\mathbf{w} + \\alpha \\big(R - \\hat{V}(\\mathbf{s})\\big) \\cdot \\mathbf{s}$$\n",
    "\n",
    "> üìù **Exercise 1.6**\n",
    ">\n",
    "> 1. üí≠ Reason why $\\mathbf{s}$ now appears in the learning rule.\n",
    "\n",
    "> üìù **Exercise 1.7**\n",
    "> \n",
    "> 1. üêçAs before, complete the `def learn(self, R, S):` function to implement the Rescorla-Wagner learning rule for multiple stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagner_multistim(BaseRescorlaWagner):\n",
    "    def __init__(self, n_stimuli=2, alpha=0.1, initial_value=0): \n",
    "        self.W = np.zeros(n_stimuli)\n",
    "        super().__init__(n_stimuli=n_stimuli, alpha=alpha)\n",
    "\n",
    "    def learn(self, S, R):\n",
    "        raise NotImplementedError(\"You need to implement this method\")\n",
    "        # V = ????  # calculate the value of the stimuli, \n",
    "        # error = # ???? # calculate the error\n",
    "        # self.W += # ???? # update the weights\n",
    "\n",
    "        # store the history\n",
    "        # self.W_history = np.vstack([self.W_history, self.W])\n",
    "        # self.R_history.append(R)\n",
    "        # self.V_history.append(S @ self.W)\n",
    "        # self.S_history.append(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def learn(self, S, R):\n",
    "    V = S @ self.W # calculate the value of the stimuli\n",
    "    error = R - V # calculate the error\n",
    "    self.W += alpha * S * error # update the weights\n",
    "\n",
    "    # store the history\n",
    "    self.W_history.append(self.W.copy())\n",
    "    self.R_history.append(R)\n",
    "    self.V_history.append(S @ self.W)\n",
    "    self.S_history.append(S)\n",
    "\n",
    "# Set the learn method to the function we just defined.\n",
    "RescorlaWagner_multistim.learn = learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.8**:\n",
    "1. üêç With your partner implement these four experiments in the Rescorla-Wagner model:\n",
    "    1. **Blocking**\n",
    "        * A single stimulus is paired with the US (A --> R), then a compound stimulus is paired with the US (AB --> R). What happens?\n",
    "    2. **Overshadowing**\n",
    "        * Two stimuli are paired with a reward (AB --> R) but one is much more salient than the other, e.g. $\\mathbf{s} = [1, 0.1]$.\n",
    "    3. **Overexpectation**\n",
    "        * Two stimuli are seperately paired with the US (A --> R, B --> R), then the compound stimulus is presented (AB --> ?). What do you observe?\n",
    "    4. **Conditioned Inhibition**\n",
    "        * A single stimulus is paired with the US (A --> R) then a second stimulus is added and the reward is removed. (AB --> _). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the blocking experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "rescorla_blocking = RescorlaWagner_multistim(n_stimuli=2, alpha=0.1)\n",
    "for i in range(50):\n",
    "    rescorla_blocking.learn(S=np.array([1,0]), R=1)\n",
    "for i in range(50):\n",
    "    rescorla_blocking.learn(S=np.array([1,1]), R=1)\n",
    "\n",
    "rescorla_blocking.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this plot: \n",
    "\n",
    "* The **top** plot shows the stimuli which were present on each trial. Their transparency denotes their strength. \n",
    "* The **middle** plot shows the association weights of the stimuli.\n",
    "* The **bottom** plot shows the total value of the presented stimuli $V(\\mathbf{s}) = \\mathbf{s}\\cdot\\mathbf{w}$ along with the reward received on each trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the overshadowing experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "rescorla_overshadowing = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(100):\n",
    "    rescorla_overshadowing.learn(S=np.array([0.9, 0.1]), R=1)\n",
    "ax = rescorla_overshadowing.plot()\n",
    "ax[0].set_title(\"Overshadowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the overexpectation experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "rescorla_overexpectation = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(50):\n",
    "    rescorla_overexpectation.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(50):\n",
    "    rescorla_overexpectation.learn(S=np.array([0, 1]), R=1)\n",
    "for i in range(50):\n",
    "    rescorla_overexpectation.learn(S=np.array([1, 1]), R=1)\n",
    "ax = rescorla_overexpectation.plot()\n",
    "ax[0].set_title(\"Overexpectation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the inhibition experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "rescorla_inhibition = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(50):\n",
    "    rescorla_inhibition.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(50):\n",
    "    rescorla_inhibition.learn(S=np.array([1, 1]), R=0)\n",
    "ax = rescorla_inhibition.plot()\n",
    "ax[0].set_title(\"Inhibition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 1.9**\n",
    ">\n",
    "> 1. üêç Starting from the inhibition experiment above, extend it so that the reward prediction goes negative.\n",
    "> 2. üêç Simulate a conditioning experiment with three stimuli, where the first two are paired with the US and the third is not. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code for the negative reward prediction experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 1]), R=0)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([0, 1]), R=1)\n",
    "ax = RW.plot()\n",
    "ax[0].set_title(\"Negative reward prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for three stimuli goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_multistim(n_stimuli=3)\n",
    "for i in range(10):\n",
    "    RW.learn(S=np.array([1, 0, 0]), R=1)\n",
    "for i in range(10):\n",
    "    RW.learn(S=np.array([1, 1, 0]), R=1)\n",
    "for i in range(10):\n",
    "    RW.learn(S=np.array([1, 1, 1]), R=1)\n",
    "ax = RW.plot()\n",
    "ax[0].set_title(\"Three stimuli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 1.10**\n",
    "> \n",
    "> 1. üí≠ TO THINK The Rescorla Wagner model fails to capture some aspects about animal conditioning. The following are three real observations about conditioning in the brain. Which would the Rescorla Wagner model you've built so far be able to capture and which wouldn't it. \n",
    "> \n",
    ">    1. A stimulus that has repeatedly been experienced alone, without a reward, is subsequently less effective at conditioning than a novel stimulus.\n",
    ">    2. If an inhibitory stimulus (i.e. one with a negative association weight) is presented on its own, without any response, it's association strength will remain stable and negative. \n",
    ">    3. If one stimulus is paired with a reward and then another is additionally introduced it will be \"blocked\" from learning. When the reward is subsequently removed the association strength of the first stimulus will begin decreasing but not the second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Temporal difference learning** <a name=\"td\"></a>\n",
    "\n",
    "One limitation of the Rescorla-Wagner model is that it doesn't take into account the temporal structure of the environment. Associations are made between stimuli _now_ and rewards _now_. In reality, rewards are often delayed. \n",
    "\n",
    "Now we consider temporally evolving \"Markov Reward Processes\" where states progress through time and may (or may not) return rewards.\n",
    "\n",
    "$$ S_{0} \\rightarrow R_{1}, S_{1} \\rightarrow R_{2}, S_{2} \\rightarrow R_{3}, \\ldots $$\n",
    "\n",
    "In a Markov reward process, the state transitions and rewards may be probabilistic (later we'll consider Markov \"Decision\" processes where transitions may depend on \"actions\" the agent chooses to take but for now we'll keep things simple). The \"value\" of a state is the expected sum of rewards that will be received in the future, starting from that state.\n",
    "\n",
    "$$V(S_t) = \\mathbb{E} \\big[ \\underbrace{R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots}_{G_t = \\textrm{``return'' from state $_{S_t}$}}\\big]$$\n",
    "\n",
    "This differs from Rescorla-Wagner only in that the value of a state is not just based on the reward received _now_ ($V(S_t) = \\mathbb{E} [R_{t+1} ]$) but also on the rewards that will be received in the future.\n",
    "\n",
    "$\\gamma$ is the discount factor, typically set near or just less than 1. This factor ensures that rewards received in the future are \"worth\" less than rewards received now. This is generally a wise assumption since rewards in the future are less certain (\"a bird in the hand is worth two in the bush\").\n",
    "\n",
    "> üìù **Exercise 2.1**\n",
    ">\n",
    "> 1. üí≠ Calculate the value of each state in the following MRP (let $\\gamma = 0.9$): \n",
    ">    $$ S_0 \\rightarrow R_1=1, S_1 \\rightarrow R_2=2, S_2 \\rightarrow R_3=3, S_3 \\rightarrow R_4=4, S_4 \\rightarrow R_5=5 (\\textrm{episode ends}) $$\n",
    "> 2. üí≠ TO THINK In a continuous task (one where the Markov process continue indefinitely and never end) why might a discount factor $\\gamma = 1$ be a problem? \n",
    "\n",
    "As before our goal is to learn an estimate of the value of each state, $\\hat{V}(S) \\leftarrow V(S)$.\n",
    "\n",
    "#### **2.0.1 Tabular vs. Function Approximation**\n",
    "\n",
    "When there are multiple stimuli (or states) in an environment you have a choice. Either:\n",
    "1. **Tabular**: You can store the value of each state in a big table (tabular learning) \n",
    "$$ \\hat{V}(S_t) = [ V(S_0), V(S_1), \\ldots, V(S_{N_{\\textrm{states}}}) ] $$\n",
    "2. **Function Approximation**: You can learn a function that maps states (typical then represented by a feature vector) to values.\n",
    "\\begin{align}\n",
    "\\hat{V}(\\mathbf{s}) & = f(\\mathbf{s})  \\hspace{4cm} \\textrm{(general)} \\\\\n",
    "& = \\mathbf{w} \\cdot \\mathbf{s}  \\hspace{4cm} \\textrm{(linear)}\\\\\n",
    "& = \\phi(\\mathbf{W}_{2}\\cdot(\\phi(\\mathbf{W}_1 \\cdot \\mathbf{s} + \\mathbf{b}_1) + \\mathbf{b}_2 )  \\hspace{1cm}\\textrm{(2-layer neural network)} \\\\\n",
    "& = \\ldots\n",
    "\\end{align}\n",
    "\n",
    "Both have advantages and disasvantages: \n",
    "- Tabular methods are typically simpler and have better convergence gaurantees. \n",
    "- Function approximation scales better to much larger state spaces, is more biologically plausible and can generalize better to unseen states.\n",
    "\n",
    "We used a linear function approximation in the Rescorla-Wagner model with multiple stimuli. This was because we wanted to be able to represent multiple stimuli simultaneously which is difficult to do with a tabular method.\n",
    "\n",
    "> üìù **Exercise 2.2**\n",
    ">\n",
    "> 1. üí≠ TO THINK Convince yourself that linear function approximation where states are represented by one-hots ($\\mathbf{s}(S_1) = [1,0,0,\\ldots], \\mathbf{s}(S_2) = [0,1,0,\\ldots], \\ldots$) is equivalent to the tabular method where $\\mathbf{w} = [V(S_1), V(S_2), \\ldots]$. Although _this_ function approximation scheme is trivial equivalent to tabular, we'll see examples later which definitely are not. \n",
    "\n",
    "For now we'll revert back to tabular methods - these will be sufficient because in Markov process you can only be in one state at any given time. In section 4 where we'll consider function approximation again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 2.3**\n",
    ">\n",
    "> 1. üí≠ **[ADVANCED]** By considering the following loss function $L_t = \\big[ \\hat{V}(S_t) - V(S_t) \\big]^2$ show, by gradient descent in $\\hat{V}(S_{t})$, that the optimal update rule for $\\hat{V}(S_t)$ is $\\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha \\big[ V(S_t) - \\hat{V}(S_t) \\big]$\n",
    "\n",
    "### **2.1 Monte-Carlo learning**<a name=\"montecarlo\"></a>\n",
    "One way to estimate the value of a state is to wait until the end of each episode, collecting all the rewards that were received along the way, and then calculate the single-episode return $G_t$ for each state and use this as a target. \n",
    "\n",
    "$$ \\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha  \\big[ G_t - \\hat{V}(S_t) \\big] $$\n",
    "\n",
    "Since the _expectation_ of $G_t$ is equal to $V(S_t)$ this update is equivalent toperforming _stochastic gradient descent_ of the loss function and is called **Monte-Carlo learning**. \n",
    "\n",
    "Although in theory this does work, in practice, Monte-Carlo learning is often infeasible because it requires waiting until the end of each episode to update the value of each state. \n",
    "\n",
    "This turns out to be quite a serious limitation in real-world applications - imagine waiting until the end of a game of chess to update the value of each state! Or worse, some environments don't have a clearly defined end such as the game of life we're all currently playing. \n",
    "\n",
    "This is where TD learning comes in..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Bootstrapping the value of a state with Temporal Difference learning**<a name=\"bootstrap\"></a>\n",
    "The key idea behind TD-learning is to estimate the value of a state by bootstrapping from the value of the next state. \n",
    "\n",
    "> üìù **Exercise 2.4**\n",
    ">\n",
    "> 1. üí≠ Show that the value of a state can be written as the sum of the reward received at that state and the value of the next state. i.e.\n",
    "> $$V(S_t) = \\mathbb{E} [R_t + \\gamma V(S_{t+1})]. $$\n",
    "\n",
    "This is called the _Bellman equation_ and is the basis of TD-learning. Its encodes a _very_ important idea: \n",
    "\n",
    "**Bellman Equation:  The future value of a state now is equal to the reward received now plus the value of the next state (discounted a little bit).**\n",
    "\n",
    "It gives us a clue for how we can bypass the need to wait until the end of each episode to update the value of each state. Just update the value of each state based on the reward received and the value of the next state (no waiting required).\n",
    "\n",
    "But wait! How do we know the value of the next state? We don't, that's why we're learning. So we'll use our current estimate of the value of the next state, $\\hat{V}(S_{t+1})$ as a _proxy_.\n",
    "\n",
    "$$V(S_t) = \\mathbb{E} [\\color{red}{\\underbrace{R_t + \\gamma V(S_{t+1})}_{\\textrm{I don't know this}}} \\color{d}{}] \\approx  \\mathbb{E}[\\color{green}{\\underbrace{R_t + \\gamma \\hat{V}(S_{t+1})}_{\\textrm{I do know this}}}\\color{d}{}] $$\n",
    "\n",
    "This gives us the TD-learning update rule:\n",
    "\n",
    "$$\\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha \\big[\\underbrace{R_t + \\gamma \\hat{V}(S_{t+1}) - \\hat{V}(S_t)}_{\\delta_t = \\textrm{``TD-error''}} \\big]$$\n",
    "\n",
    "The term $\\delta_t$ is the _temporal difference error_. It can be high (low) for two reasons: \n",
    "1. The reward was better (or worse) than expected.\n",
    "2. The value of the next state was higher (or lower) than expected.\n",
    "\n",
    "The second point is crucial: even though we might not observe reward in a given state we may still assign this state value because the _next_ state has value. This is called _bootstrapping_. It's the same reason you might be happy to have received an invitation to a job interview even though interviews aren't inherently fun - you're bootstrapping from the value of the next state (potentially being offered a job)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 2.5**\n",
    ">\n",
    "> 1. üêç Implement the TD-learning update rule in the `def learn(self, R, S, S_next, alpha):` function in the `TDLearner` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLearner(BaseTDLearner):\n",
    "    def __init__(self, gamma=0.5, alpha=0.1, n_states=10):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.V = np.zeros(n_states)\n",
    "        super().__init__(gamma=gamma, alpha=alpha, n_states=n_states)\n",
    "        \n",
    "    def learn(self, S, S_next, R):\n",
    "        raise NotImplementedError(\"You need to implement this method\")\n",
    "        # Get's the value of the current and next state\n",
    "        V = self.V[S]             if S is not None else 0\n",
    "        V_next = self.V[S_next]   if S_next is not None else 0\n",
    "        \n",
    "        # Calculates the TD error (hint remember to use self.gamma\n",
    "        # TD_error = # ???\n",
    "\n",
    "        # Updates the value of the current state\n",
    "        # if S is not None:\n",
    "            # self.V[S] = # ???\n",
    "\n",
    "        #  return TD_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def learn(self, S, S_next, R):\n",
    "    # Get's the value of the current and next state\n",
    "    V = self.V[S]              if S is not None else 0\n",
    "    V_next = self.V[S_next]    if S_next is not None else 0\n",
    "    \n",
    "    # Calculates the TD error \n",
    "    TD_error = R + self.gamma * V_next - V\n",
    "\n",
    "    # Updates the value of the current state\n",
    "    if S is not None:\n",
    "        self.V[S] = self.V[S] + self.alpha * TD_error\n",
    "        \n",
    "    return TD_error\n",
    "\n",
    "TDLearner.learn = learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Training a TD-Learner on a sequence of states** \n",
    "\n",
    "Now we're going to train the TD learner in a simple markov reward process environment. We'll use the following class method (already written for, not shown)\n",
    "\n",
    "- `TDLearner.learn_episode(states, rewards)` takes a list of `states` = $[S_0, S_1, \\ldots, S_T]$ and  `rewards` = $[R_1, R_2, \\ldots, R_{T+1}]$ and trains the TD-learner on this sequence of states and rewards by calling the `learn` method at each time step. It then saves the results to history. \n",
    "\n",
    "_**Note** this method appends as state $S_{-1} = $ `None` and reward $R_0 = 0$ to the start of the sequence. This represents the initial state and reward before the first state is observed, and it cannot be predicted (it will be useful in exercise 2.8.3))_\n",
    "\n",
    "For example, an MRP where states deterministically progress from 0 to 1 to ... to 9 and receiving a reward of zero except at state 9 where a reward of 1 is received can be simulated as follows:\n",
    "\n",
    "```python\n",
    "TD.perform_episode(\n",
    "    states  = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
    "    rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
    ")\n",
    "```\n",
    "\n",
    "> üìù **Exercise 2.6**\n",
    ">\n",
    "> 1. üêç Simulate 100 episodes of the above MRP (let $\\alpha = 0.5$ and $\\gamma = 0.9$) then use the `TDLearner.plot(episode=0)` method to show the results after the first and last episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Set some parameters \n",
    "n_episodes = 50\n",
    "gamma = 0.90\n",
    "alpha = 0.5 # for now use a high learning rate\n",
    "n_states = 10\n",
    "\n",
    "# Initialize the TD learner\n",
    "tdlearner = TDLearner(gamma=gamma, n_states=n_states, alpha=alpha)\n",
    "\n",
    "# Generate the MRP \n",
    "states = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "\n",
    "# Run the experiment\n",
    "for episode in range(n_episodes):\n",
    "    tdlearner.learn_episode(\n",
    "        states=states,\n",
    "        rewards=rewards,)\n",
    "\n",
    "# Plot the results\n",
    "anim = TD.animate_plot()\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets discuss these plot: \n",
    "\n",
    "- The **top-left** plot shows the states (y-axis) visited across time (x axis)\n",
    "- The **top-right** plot shows the current state value estimate after this episode. \n",
    "- The **bottom** plots shows the reward received at each state. and the TD error at each state.\n",
    "\n",
    "We can also animate _all_ the episodes using the `TD_ValueLearner.animate_plot()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the firing of dopamine neurons in the brain responding initially to the reward and the to earlier and earlier reward predicting stimuli. \n",
    "\n",
    "> üìù **Exercise 2.7**\n",
    ">\n",
    "> 1. üí≠ After learning has converged, what is the value of state 0? \n",
    "> 2. üí≠ Approximately how \"fast\" does the value bump move backwards? How does this relate to the notion of one-step bootstrapping. \n",
    "> 3. üí≠ Why does a residual TD-error accumulate at the start? Understanding this is important for understanding TD-learning.\n",
    "> 1. üí≠ TO THINK Discuss the similarities and differences between the backward movement of TD error in your model and the backward movement of dopamine firin in the brain. \n",
    "> 2. üêç **[ADVANCED]** Perhap you noticed that the TD spreads out as it moves backwards. Think about why this happens, play around with the learning rate $\\alpha$ and see if this changes things. \n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/td_dopamine.png\" width=500>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 2.8**\n",
    ">\n",
    "> 1. üí≠  What would happen to the TD error if a small positive reward is given at the end (as we just simulated) and then, after learning has converged, the reward is removed? _Hint: think about what happens in the brain, and why_.\n",
    "> 2. üí≠ Calculate the TD error which would be observed in the terminal state $S=9$ the first instance the reward is removed. \n",
    "> 3. üêç Simulate this experiment. \n",
    "> 4. üí≠ TO THINK Discuss the similarities between this and the following figure showing the firing of dopamine neurons in the brain when a previously stable reward is removed.\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/td_noreward.png\" width=300>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "# Set some parameters \n",
    "n_episodes = 50\n",
    "gamma = 0.95\n",
    "alpha = 0.5 # for now use a high learning rate\n",
    "n_states = 10\n",
    "\n",
    "# Initialize the TD learner\n",
    "tdlearner = TDLearner(gamma=gamma, n_states=n_states, alpha=alpha)\n",
    "\n",
    "# Generate the MRP \n",
    "states = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "later_rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Run the experiment\n",
    "for episode in range(n_episodes):\n",
    "    tdlearner.learn_episode(\n",
    "        states=states,\n",
    "        rewards=rewards,)\n",
    "# Run the experiment\n",
    "for episode in range(n_episodes):\n",
    "    tdlearner.learn_episode(\n",
    "        states=states,\n",
    "        rewards=later_rewards,)\n",
    "\n",
    "# Animating the results\n",
    "anim = TD.animate_plot()   \n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 2.9 [ADVANCED]**\n",
    "> \n",
    "> 1. üêç What happens if this environment is stochastic? Adapt the code to model the following very stochastic environment...\n",
    ">\n",
    ">   - **The state transitions are now probabilistic**: \n",
    ">        - As before there are $N = 10$ states from $i=0$ to $i=N-1$.\n",
    ">        - When the agent is in state $S_t = i$ it has a probability $p_t = 0.8$ of moving to state $S_{t+1} = i+1$ and a 0.1 probability of staying in state $S_{t+1} = i$. \n",
    ">       - The agent starts in state $S =0$ and episode ends whenever the agent reaches state $S=N-1$.\n",
    ">\n",
    ">   - **The rewards are probabilistic** as a function of the state: \n",
    ">        - When the agent is in state $S_t = i$ a reward is randomly drawn from a normal distribution $R_t = R(S_t=i) \\sim \\mathcal{N}(\\mu=(i+1) / N, \\sigma=1)$. \n",
    ">\n",
    ">   - **Simulate** 200 episodes(let $\\gamma = 0.8$ and $\\alpha = 0.1$ of this new MRP and plot or animate the results.\n",
    ">\n",
    "> 2. üí≠ Though stochastic, there is a exact solution to the value of each state in this environment. Can you derive it? \n",
    "> You'll want to start from the Bellman equation $V(S_t) = \\mathbb{E} [R_t + \\gamma V(S_{t+1})]$ and show the expected value of the last state $S = N-1$ is 1.0 then derive the following recursion relation:\n",
    "> $$ V(N-1) = 1$$\n",
    "> $$ V(n) = \\frac{1}{1 - \\gamma (1-p_t)} \\left( \\frac{n + 1}{N} + \\gamma p_t \\cdot V(n+1) \\right) $$ \n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution (warning: may take up to a minute to animate) {display-mode: \"form\" }\n",
    "# # Set some parameters \n",
    "n_episodes = 200\n",
    "gamma = 0.8\n",
    "alpha = 0.1\n",
    "n_states = 10\n",
    "p_transition = 0.8 \n",
    "\n",
    "# Initialize the TD learner\n",
    "tdlearner = TDLearner(gamma=gamma, n_states=n_states, alpha=alpha)\n",
    "\n",
    "\n",
    "# Run the experiment\n",
    "for episode in range(n_episodes):\n",
    "    # Generate the MRP \n",
    "    states = np.array([0])\n",
    "    rewards = np.array([0])\n",
    "    while states[-1] != 9:\n",
    "        # Stochoastic transition\n",
    "        if np.random.rand() < p_transition: state_next = states[-1] + 1\n",
    "        else: state_next = states[-1]\n",
    "        # Stochoastic reward\n",
    "        # p_reward = np.random.normal(loc=(states[-1]+1) / n_states)\n",
    "        # reward = np.random.choice([0, 1], p=[1-p_reward, p_reward])\n",
    "        reward = np.random.normal(loc=(states[-1]+1) / n_states)\n",
    "        rewards = np.append(rewards, reward)\n",
    "        states = np.append(states, state_next)\n",
    "    tdlearner.learn_episode(\n",
    "        states=states,\n",
    "        rewards=rewards,)\n",
    "\n",
    "# NEW Here we calculate the theoretical value estimates and set them to be plotted\n",
    "theoretical_value = np.zeros(n_states)\n",
    "for i in range(n_states)[::-1]:\n",
    "    if i == n_states - 1:\n",
    "        theoretical_value[i] = 1 \n",
    "    else:\n",
    "        theoretical_value[i] = (1 / (1 - gamma * (1 - p_transition))) * (((i + 1)/n_states) + gamma * p_transition * theoretical_value[i+1])\n",
    "tdlearner.theoretical_value = theoretical_value #this will plot the theoretical value onto the animation \n",
    "\n",
    "\n",
    "# Plot the results\n",
    "anim = TD.animate_plot()\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dotted line `---` on the value curve (top right) now displays the theoretical solution.\n",
    "\n",
    "**Observation:** Even though the environment is highly stochastic the _expected_ value of each state is well defined. With a small learning rate the TD learner can learn a stable estimate this expected value essentially smoothing over the stochasticity. If you repeat this experiment with a higher learning rate, you'll see the value estimates are much more noisy.\n",
    "\n",
    "> üìù **Exercise 2.10** \n",
    "> \n",
    "> 1. üí≠ TO THINK: Discuss how this observation relates to learning in the brain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **3. Q-Values and Policy Improvement** <a name=\"q\"></a>\n",
    "\n",
    "So far we have only considered environments where there is no choice. The agent is simply moving through a sequence of states. In the real world agents have choices (called _actions_).\n",
    "\n",
    "For this we need to introduce the idea concept of a _policy_. A policy is any function which maps states to actions, actions then determine how one state leads to another. Policies are often denoted by $\\pi$:\n",
    "\n",
    "$$ \\pi : S \\rightarrow A $$\n",
    "\n",
    "A _Markov reward process_ (MRP) where the agent has a policy is called a _Markov Decision Process_ (MDP).\n",
    "\n",
    "$$ S_0 \\xrightarrow{A_0 \\sim \\pi(S_0)} R_1, S_1 \\xrightarrow{A_1\\sim \\pi(S_1)} R_2, S_2 \\xrightarrow{A_2\\sim \\pi(S_2)} R_3, S_3 \\xrightarrow{A_3\\sim \\pi(S_3)} R_4, S_4 \\xrightarrow{A_4\\sim \\pi(S_4)} \\ldots $$\n",
    "\n",
    "The _action_ the agent took at each state determines the state the agent ends up in next which may (or may not) come with a reward.\n",
    "\n",
    "### **3.1 Q-Values**<a name=\"qvalues\"></a>\n",
    "\n",
    "The \"value'' of a state now depends not just on the state but on the action taken in that state and the policy that was followed thereafter. \n",
    "\n",
    "Q-values represent a natural generalisation of the state value of a state-action pair under a policy: \n",
    "\n",
    "$$ Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} \\big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\big| S_t = s, A_t = a \\big] $$\n",
    "\n",
    "In plain English this say: \"$Q_{\\pi}(s, a)$ is the expected return from taking action $a$ in state $s$ and then following policy $\\pi$.\". Note the expectation is over $\\pi$ because the policy determines the future actions, states and rewards. The policy (which actions are taken in which states) and the environment (which states and rewards are reached from which states) could _both_ be stochastic.\n",
    "\n",
    "Like the value of a state, the Q-values satisfy the Bellman equation:\n",
    "\n",
    "$$ Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} \\big[ R_{t+1} + \\gamma Q_{\\pi}(S_{t+1}, A_{t+1}) \\big| S_t = s, A_t = a, A_{t+1}\\sim\\pi(S_{t+1}) \\big] $$\n",
    "\n",
    "> üìù **Exercise 3.1 [ADVANCED]**\n",
    ">\n",
    "> <center> <img src=\"./figures/mdp.png\" width=300> </center>\n",
    ">\n",
    "> 1. üí≠ Consider the above simple MDP where there are two states ($S_1$ and $S_2$) and two actions ($A_1$ and $A_2$).\n",
    ">       - $S_1$: taking action $A_1$ leads to $S_2$ with reward 2, taking action $A_2$ leads back to $S_1$ with reward 1.\n",
    ">       - $S_2$: taking action $A_1$ leads to $S_1$ with reward 3, taking action $A_2$ leads back to $S_2$ with reward 1.\n",
    "> \n",
    ">    1. üí≠ Given the policy $\\pi_1$ where $\\pi_1(S_1) = A_1$, $\\pi(S_2) = A_2$, what are the Q-values of each state-action pair under this policy (it might help to calculate them in the following order)?\n",
    ">       * $Q_{\\pi_1}(S_2, A_2)$\n",
    ">       * $Q_{\\pi_1}(S_1, A_1)$\n",
    ">       * $Q_{\\pi_1}(S_1, A_2)$\n",
    ">       * $Q_{\\pi_1}(S_2, A_1)$\n",
    ">\n",
    ">    2. üí≠ Write down the optimal policy $\\pi^*$ (Hint: don't overthink it) \n",
    ">       * $\\pi^*(S_1) = ?$\n",
    ">       * $\\pi^*(S_2) = ?$\n",
    ">\n",
    ">    3. üí≠ What are the Q-values of each state-action pair under the optimal policy $\\pi^*$?\n",
    ">       * $Q_{\\pi^*}(S_1, A_1)$\n",
    ">       * $Q_{\\pi^*}(S_2, A_2)$\n",
    ">       * $Q_{\\pi^*}(S_1, A_2)$\n",
    ">       * $Q_{\\pi^*}(S_2, A_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 SARSA**<a name=\"sarsa\"></a>\n",
    "\n",
    "Not all MDPs are  simple enough to solve analytically like the example above. In practice we often use _learning algorithms_ to estimate the Q-values from observations. \n",
    "\n",
    "The closest equivalent learning rule to TD-learning rule for Q-values is:\n",
    "\n",
    "$$\\hat{Q}_{\\pi}(S_t, A_t) \\leftarrow \\hat{Q}_{\\pi}(S_t, A_t) + \\alpha \\big[ R_{t+1} + \\gamma \\hat{Q}_{\\pi}(S_{t+1}, A_{t+1}) - \\hat{Q}_{\\pi}(S_t, A_t) \\big]$$\n",
    "\n",
    "This is often called the SARSA learning rule because it takes into account the **S**tate $S_t$, the **A**ction $A_t$, the **R**eward $R_{t+1}$, the next **S**tate $S_{t+1}$ and the next **A**ction $A_{t+1}$.\n",
    "\n",
    "Note how it looks almost identical to the TD-learning rule for state values but with the addition of the action terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adapt or `TDLearner` class into a `TDQLearner` to learn Q-values. It's very simple, we just need to... \n",
    "\n",
    "1. ...change `self.V` (a list) to `self.Q` (an array) to store Q-values for each state-action pair. \n",
    "    - `self.Q[s, a]` is the Q-value of state `s` and action `a`.\n",
    "2. ...change the `learn()` method to update Q-values instead of state values. I.e. instead of \n",
    "    - `self.learn(S, S_next, R)` updating `self.V[S]`...\n",
    "    - `self.learn(S, S_next, A, A_next, R)` should update `self.Q[S, A]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 3.2**\n",
    ">\n",
    "> 1. üêç Complete the missing lines (those with `????`) in the `def learn(self, S, S_next, A, A_next, R, alpha):` function in the `TD_QValueLearner` class below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDQLearner(BaseTDQLearner):\n",
    "    def __init__(self, gamma=0.5, alpha=0.1, n_states=10, n_actions=2):\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__(gamma=gamma, alpha=alpha, n_states=n_states)\n",
    "  \n",
    "    def learn(self, S, S_next, A, A_next, R):\n",
    "        # Get's the value of the current and next state\n",
    "        raise NotImplementedError(\"You need to implement this method\")\n",
    "        # Q = # ???? # get the value of the current state (remember it's zero if S is None)\n",
    "        # Q_next = # ???? # get the value of the next state\n",
    "         \n",
    "        # Calculates the TD error (hint remember to use self.gamma\n",
    "        # TD_error = # ???? # calculate the TD error\n",
    "\n",
    "        # Updates the value of the current state\n",
    "        # if S is not None:\n",
    "        #     self.Q[S,A] = # ???? # update the Q value\n",
    "\n",
    "        # return TD_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, S, S_next, A, A_next, R):\n",
    "    # Get's the value of the current and next state\n",
    "    Q = self.Q[S,A] if S is not None else 0\n",
    "    Q_next = self.Q[S_next, A_next] if S_next is not None else 0\n",
    "    \n",
    "    # Calculates the TD error (hint remember to use self.gamma\n",
    "    TD_error = R + self.gamma * Q_next - Q\n",
    "\n",
    "    # Updates the value of the current state\n",
    "    if S is not None:\n",
    "        self.Q[S,A] = self.Q[S,A] + self.alpha * TD_error \n",
    "\n",
    "    return TD_error\n",
    "TDQLearner.learn = learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 3.3**\n",
    "> \n",
    "> 1. üêç Perform the following experiment: Each episode, with a 50:50 probability, is either a \"Left\" episode or a \"Right\" episode:\n",
    ">       - **\"Right\":** Agent moves to the right from $S=0$ to $S=9$ (action $A = 0$), receiving a reward ($R=1$) at the terminal state.\n",
    ">       - **\"Left\"** Agent moves to the left from $S=9$ to $S=0$ (action $A = 1$), receiving a negative reward ($R=-1$) at the terminal state.\n",
    "> 2. üêç Once simulated, plot the Q-values (stored in `TD_QValueLearner.Q`) of each state-action pair. What difference do you observe between the Q-values of _the same two states_ under the two different actions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this code \n",
    "gamma = 0.90 # discount factor\n",
    "alpha = 0.5 # learning rate\n",
    "n_episodes = 100\n",
    "\n",
    "# Make the TDQ learner\n",
    "tdqlearner = TDQLearner(gamma=gamma, alpha=alpha, n_states=10, n_actions=2)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    \n",
    "    # randomly choose between a left or right episode\n",
    "    episode_type = np.random.choice(['left', 'right'])\n",
    "\n",
    "    # write code which generates the states, actions and rewards for the episode and then performs the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "gamma = 0.90 # discount factor\n",
    "alpha = 0.5 # learning rate\n",
    "n_episodes = 100\n",
    "\n",
    "# Make the TDQ learner\n",
    "tdqlearner = TDQLearner(gamma=gamma, alpha=alpha, n_states=10, n_actions=2)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    \n",
    "    # randomly choose between a left or right episode\n",
    "    episode_type = np.random.choice(['left', 'right'])\n",
    "\n",
    "    if episode_type == 'right':\n",
    "        states = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        actions = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "    elif episode_type == 'left':\n",
    "        states = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n",
    "        actions = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "        rewards = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, -1])\n",
    "    \n",
    "    tdqlearner.learn_episode(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to plot the Q values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "Q_values = tdqlearner.Q\n",
    "fig, ax = plt.subplots(1,1 , figsize=(4,2))\n",
    "ax.bar(np.arange(10), Q_values[:,0], color='C0', label='Action = 0 (\"Right\")', alpha=0.5)\n",
    "ax.bar(np.arange(10), Q_values[:,1], color='C1', label='Action = 1 (\"Left\")', alpha=0.5)\n",
    "ax.axhline(0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('State')\n",
    "ax.legend() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Policy Improvement**<a name=\"policyimprovement\"></a>\n",
    "\n",
    "So far we have learnt: \n",
    "1. **Rewards:** How to associate states with rewards by minimising a prediciton error (Rescorla-Wagner)\n",
    "2. **Values:** Associate a _state_ with future rewards by learning it's value (TD-learning)\n",
    "3. **Q-Values** Evaluate a _policy_ by calculating the Q-value of taking each action in each state (SARSA). \n",
    "\n",
    "The final pillar of model-free reinforcement learning is **Policy Improvement**. \n",
    "\n",
    "4. **Policy Improvement** Use `<some-algorithm>` to (iteratively) improve the policy towards the optimal policy. \n",
    "\n",
    "The _optimal_ policy (which we'd like to find) is defined as the one which maximises the expected return from each state:\n",
    "$$ \\pi^* = \\arg\\max_{\\pi} Q_{\\pi}(s, a) \\hspace{3mm} \\forall \\hspace{3mm}s, a$$\n",
    "\n",
    "There two main families of algorithms for finding optimal policies in RL:\n",
    "1. **Policy-based methods**: Learn the policy function directly, for example using a neural network to map states to actions directly (example: _REINFORCE_, or _Actor-Critic_ methods). We will not be covering these methods in this tutorial.\n",
    "1. **Value-based methods**: Learn the Q-values and then derive the policy from the Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3.1 Greedy policy**\n",
    "\n",
    "So how do we use $Q_{\\pi}(s,a)$ to improve the policy? Suppose we define a new policy \n",
    "\n",
    "$$ \\textrm{Greedy policy:}\\hspace{1cm}\\pi^{\\prime}(s) = \\arg\\max_a Q_{\\pi}(s, a) $$\n",
    "\n",
    "A very important theorem in RL called the _policy improvement theorem_ states that $\\pi^{\\prime}$ is guaranteed to be as good as or better than $\\pi$ in all states. \n",
    "\n",
    "That is...if we know the Q-values of a given policy we can find a new policy which will return at least as much reward, if not more, in all states by simply choosing the action with the highest Q-value.\n",
    "\n",
    "This is an immensely powerful result and although we won't prove it today it's easy to intuitively understand why it's true: if you know the value of each action in each state you can simply choose the best one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3.2 Exploration vs. Exploitation**\n",
    "\n",
    "There's one problem with the greedy policy: it's greedy! It always chooses the action with the highest Q-value. This is great if you know the Q-values perfectly but in practice you often don't.\n",
    "\n",
    "In practice you need to balance _exploration_ (trying new things) with _exploitation_ (doing what you know works). This is a fundamental trade-off in reinforcement learning and is a key challenge in designing reinforcement learning algorithms.\n",
    "\n",
    "One simple way to do this is to use an $\\epsilon$-greedy policy. This is a policy which with probability $1-\\epsilon$ chooses the greedy action and with probability $\\epsilon$ chooses a random action. This allows \n",
    "\n",
    "$$ \\epsilon \\textrm{-greedy policy:}\\hspace{1cm}\\pi^{\\prime}(s) = \\begin{cases}\n",
    "\\arg\\max_a Q_{\\pi}(s, a) & \\textrm{with probability } 1-\\epsilon \\\\\n",
    "\\textrm{random action} & \\textrm{with probability } \\epsilon\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 Policy Iteration**<a name=\"policyiteration\"></a>\n",
    "\n",
    "So we know how to evaluate a policy with an algorithm such as SARSA, and we know how to improve a policy by taking the greedy action in each state. Now all we need to do is to combine these two steps in an iterative algorithm called _policy iteration_.\n",
    "\n",
    "1. Evaluating the policy (learning the Q-values under the policy)\n",
    "2. Improving the policy (using the Q-values to derive a new policy)\n",
    "\n",
    "This is a simple algorithm which is guaranteed to converge to the optimal policy.\n",
    "\n",
    "<center><img src=\"./figures/policy_iteration.png\" width=400></center>\n",
    "\n",
    "\n",
    "> üìù **Exercise 3.4**\n",
    ">\n",
    "> 1. üí≠ The exploitation-exploration trade-off is a key challenge in reinforcement learning. Can you think of a real-world example where this trade-off is important?\n",
    "> 2. üí≠ In a stable environment which you know well and is not subject to change, would it be better to prioritise exploration or exploitation? Can you think of an example of such an environment in the real world?\n",
    "> 3. üí≠ What about in an unstable environment which you don't know well and is constantly changing?\n",
    "> 4. üí≠ Greedy policies are always deterministic. Can you think of some real-world examples where a stochastic policy might be optimal? Note: we are not referring to examples where exploration helps you to learning in an uncertain system, but where a stochastic policy is fundamentally optimal even once you know the system as well as possible.\n",
    "> 5. üí≠ Can you think of a different way to balance exploration and exploitation other than $\\epsilon$-greedy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 Grid world**<a name=\"gridworld\"></a>\n",
    "\n",
    "In the following section we're going to construct a simple grid world environment and train a TD-learner to learn the Q-values of each state-action pair then use policy iteration to derive the optimal policy.\n",
    "\n",
    "#### **3.4.1 Initialising a grid world environment**\n",
    "\n",
    "```python\n",
    "environment = MiniGrid(\n",
    "    grid = grid, # a numpy array representing the grid world\n",
    "    reward_locations = [(6,6),], # a list of tuples representing the location of rewards\n",
    "    reward_values = [1,] # a list of the reward values at each location\n",
    "    )\n",
    "```\n",
    "The agent will automatically be initialised in a random position. \n",
    "\n",
    "The two most important functions you will write are are: \n",
    "1. `policy(state, Q_values)` which returns an action.\n",
    "2. `environment.step(action)` which takes an action and returns the next state and reward and whether the episode has ended.\n",
    "\n",
    "#### **3.4.2 Indexing grid world states and actions (boring but important)**\n",
    "The \"state\" is the position of the agent in the 2D environment: \n",
    "* `environment.agent_pos --> (x,y) tuple` e.g. `(3,4)` is the 3rd grid from the left, fourth grid from the bottom. \n",
    "* `environment.pos_to_state(pos) --> int` a unique single integer representing the state, for a 10 x 5 grid this would be `(9-environment.agent_pos[1]) * 10 + environment.agent_pos[0]`.\n",
    "\n",
    "There are four possible actions:\n",
    "* `0` = move North, `1` = move East, `2` = move South, `3` = move West.\n",
    "\n",
    "Some other useful methods and attributes:\n",
    "* `environment.reset()` resets the agent to a random position.\n",
    "* `environment.pos_to_state(pos)` converts a position to a state.\n",
    "* `environment.is_wall(pos)` returns `True` if the position is a wall.\n",
    "* `environment.is_reward(pos)` returns the reward value found at the position.\n",
    "* `environment.n_states` the number of states in the environment.\n",
    "* `environment.n_actions` the number of actions in the environment.\n",
    "\n",
    "Plotting functions: \n",
    "* `environment.render()` renders the current state of the environment.\n",
    "* `environment.plot_Q(Q)` plots the Q-values of each state-action pair.\n",
    "* `environment.plot_episode(episode : int)` plots the `episode`_th episode from the history. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates a small grid world ,play around a bit to get a feel for how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array( # 0 = empty, 1 = wall\n",
    "    [\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],        \n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ])\n",
    "\n",
    "environment = MiniGrid(\n",
    "    grid = grid,\n",
    "    reward_locations = [(6,6),],\n",
    "    reward_values = [1,]\n",
    "    )\n",
    "\n",
    "ax = environment.render()\n",
    "ax.set_title(\"A minigrid environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 3.5**\n",
    ">\n",
    "> 1. üêç Write the step function for the grid world environment. The rules of the environment should be as follows: \n",
    ">       - If the agent steps into a wall it should stay in the same place and receive a reward of -1.\n",
    ">       - If the agent steps into the goal it should receive the reward associated with that goal and the episode should end.\n",
    ">       - Otherwise the agent should receive a reward of -0.1 (cost of moving around)\n",
    "> 2. üí≠ TO THINK Discuss why a slightly negative cost to movement is useful to \n",
    ">       - encourage the agent to find the _shortest_ path to the goal\n",
    ">       - encourage the agent to explore the environment early on when Q-values are initialised to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action : int):\n",
    "    \"\"\"This function must have a very specific signature to work with the MiniGrid environment. \n",
    "    1. It must update the self.agent_pos attribute tuple from (x, y) to (x_new, y_new) based on the action\n",
    "    2. It must return the following tuple (state, reward, is_terminal) where:\n",
    "        - state is the new agent position (x_new, y_new)\n",
    "        - reward is the reward at the new agent position\n",
    "        - is_terminal is a boolean indicating if the agent has reached the terminal state. \n",
    "    \n",
    "    It might make sense to break it down into two steps: \n",
    "    (i) propose a new position based on the action\n",
    "    (ii) check if the new position is a wall or reward then update the agent position return the reward and terminal status. \n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"You need to implement the step method\")\n",
    "    \n",
    "    # Propose a new position based on the action\n",
    "    proposed_new_pos = None # write this \n",
    "\n",
    "    # Check if the new position is a wall or reward \n",
    "    is_wall = self.is_wall(proposed_new_pos) # returns True if the proposed new position is a wall\n",
    "    if not is_wall:\n",
    "        self.agent_pos = proposed_new_pos\n",
    "    \n",
    "    # Check if the new position is a reward\n",
    "    reward = self.get_reward(proposed_new_pos) # returns any reward found at the proposed new position\n",
    "    is_terminal = (reward > 0) # If a reward is found then the episode is over\n",
    "    reward += None # consider adding other contributions to the reward from moving costs or hitting walls\n",
    "\n",
    "    # Get the new state \n",
    "    state = self.pos_to_state(self.agent_pos)\n",
    "\n",
    "    return self.agent_pos, reward, is_terminal \n",
    "\n",
    "MiniGrid.step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def step(self, action):\n",
    "    # Propose a new position based on the action\n",
    "    proposed_new_pos = None # write this \n",
    "    if action == 0:   delta = (0,1) # North\n",
    "    elif action == 1: delta = (1,0) # East\n",
    "    elif action == 2: delta = (0,-1) # South\n",
    "    elif action == 3: delta = (-1,0) # West\n",
    "   \n",
    "    # Get the proposed next position by adding the delta to the current position\n",
    "    proposed_new_pos = np.array(self.agent_pos) + np.array(delta)\n",
    "    proposed_new_pos = tuple(proposed_new_pos)\n",
    "\n",
    "    # Check if the new position is a wall or reward \n",
    "    is_wall = self.is_wall(proposed_new_pos) # returns True if the proposed new position is a wall\n",
    "    if not is_wall:\n",
    "        self.agent_pos = proposed_new_pos\n",
    "        self.agent_direction = action\n",
    "\n",
    "    # Check if the new position is a reward\n",
    "    reward = self.get_reward(proposed_new_pos) # returns True if the proposed new position is a reward\n",
    "    is_terminal = (reward > 0) # If a reward is found then the episode is over\n",
    "    reward += -self.cost_per_step # cost of moving (defaults to 0.1)\n",
    "    if is_wall: reward += -self.cost_per_wall_collision # cost of hitting a wall (defaults to 1)\n",
    "\n",
    "    # Get the new state \n",
    "    state = self.pos_to_state(self.agent_pos)\n",
    "\n",
    "    return state, reward, is_terminal\n",
    "\n",
    "MiniGrid.step = step # set the step method to the function we just defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it works. Here we just take a few random steps and observe the agent moving around the grid world as well as any rewards it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = MiniGrid(\n",
    "    grid = grid,\n",
    "    reward_locations = [(10,2),]\n",
    "    )\n",
    "fig, ax = plt.subplots(1,10, figsize=(10,1))\n",
    "for i in range(10): # take 10 random actions and plot them \n",
    "    action = np.random.randint(0,4)\n",
    "    state, reward, terminal = environment.step(action)\n",
    "    ax[i] = environment.render(ax=ax[i])\n",
    "    ax[i].set_title(f'{dict(enumerate([\"‚Üë\", \"‚Üí\", \"‚Üì\", \"‚Üê\"]))[action]}, R = {reward}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 3.6**\n",
    "> \n",
    "> 1. üêç Write a policy function which takes the Q-values of a state and returns the greedy action.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def epsilon_greedy_policy(Q_values, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q_values are the Q values for each possible action in the current state, shape = (n_actions,)\n",
    "    epsilon is the probability of selecting a random action\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"You need to implement the policy method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def epsilon_greedy_policy(Q_values, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q_values are the Q values for each possible action in the current state, shape = (n_actions,)\n",
    "    epsilon is the probability of selecting a random action\n",
    "    \"\"\"\n",
    "    n_actions = Q_values.shape[0]\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        action = np.argmax(Q_values)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4.3 Training the grid world**\n",
    "\n",
    "The following method trains a `TDQLearner` on the grid world environment. Make sure you understand how it works. THis functino is important, it wraps up everything we've learnt so far into a training loop:\n",
    "\n",
    "1. An episode is started by resetting the environment and the agent's position.\n",
    "2. An episode is performed until the agent reaches the goal or the maximum number of steps is reached:\n",
    "    1. The agent chooses an action using the policy function and the `TDQLearner.Q` values.\n",
    "    2. The agent takes the action and receives the next state and reward.\n",
    "    3. The agent learns from the transition using the `TDQLearner.learn()` method.\n",
    "3. Everything is saved for later plotting \n",
    "\n",
    "```python\n",
    "\n",
    "> üìù **Exercise 3.7**\n",
    ">\n",
    "> 1. üêç Complete the missing lines marked `# ????`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    self,\n",
    "    tdqlearner, \n",
    "    n_episodes=1000,\n",
    "    max_episode_length=100,\n",
    "    policy=epsilon_greedy_policy,\n",
    "    ): \n",
    "\n",
    "    for i in (pbar := tqdm(range(n_episodes))):\n",
    "\n",
    "        try: # this just allows you to stop the loop by pressing the stop button in the notebook\n",
    "            \n",
    "            # Initialise an episode: \n",
    "            terminal = False\n",
    "            self.reset() # reset the environment\n",
    "            state = self.pos_to_state(self.agent_pos)\n",
    "            action = policy(tdqlearner.Q[state])\n",
    "\n",
    "            episode_data = {'positions': [], 'states':[], 'actions':[], 'rewards':[]}\n",
    "            \n",
    "            step = 0\n",
    "            while not terminal and step < max_episode_length:\n",
    "                # Get the next state and reward using the step() method\n",
    "                # state_next, reward, terminal = ???? \n",
    "                \n",
    "                # Get the next action using the policy() function\n",
    "                # next_action = ????\n",
    "                \n",
    "                # Learn from the transition using the tdqlearner.learn() method\n",
    "                # tdqlearner.learn( ???? \n",
    "\n",
    "                # store the data\n",
    "                episode_data['positions'].append(self.agent_pos)\n",
    "                episode_data['states'].append(state)\n",
    "                episode_data['actions'].append(action)\n",
    "                episode_data['rewards'].append(reward)\n",
    "\n",
    "                # update the state and action\n",
    "                state = state_next\n",
    "                action = next_action \n",
    "                step += 1\n",
    "            \n",
    "            self.episode_history[minigrid.episode_number] = episode_data\n",
    "            minigrid.episode_number += 1\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "MiniGrid.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "def train(\n",
    "    self,\n",
    "    tdqlearner, \n",
    "    n_episodes=1000,\n",
    "    max_episode_length=100,\n",
    "    policy=epsilon_greedy_policy,\n",
    "    ): \n",
    "\n",
    "    for i in (pbar := tqdm(range(n_episodes))):\n",
    "\n",
    "        try: # this just allows you to stop the loop by pressing the stop button in the notebook\n",
    "            \n",
    "            # Initialise an episode: \n",
    "            terminal = False\n",
    "            self.reset() # reset the environment\n",
    "            state = self.pos_to_state(self.agent_pos)\n",
    "            action = policy(tdqlearner.Q[state])\n",
    "\n",
    "            episode_data = {'positions': [], 'states':[], 'actions':[], 'rewards':[]}\n",
    "            \n",
    "            step = 0\n",
    "            while not terminal and step < max_episode_length:\n",
    "                # Get the next state and reward using the step() method\n",
    "                state_next, reward, terminal = self.step(action)\n",
    "                \n",
    "                # Get the next action using the policy() function\n",
    "                next_action = policy(tdqlearner.Q[state_next])\n",
    "                \n",
    "                # Learn from the transition using the tdqlearner.learn() method\n",
    "                tdqlearner.learn(state, state_next, action, next_action, reward)\n",
    "\n",
    "                # store the data\n",
    "                episode_data['positions'].append(self.agent_pos)\n",
    "                episode_data['states'].append(state)\n",
    "                episode_data['actions'].append(action)\n",
    "                episode_data['rewards'].append(reward)\n",
    "\n",
    "                # update the state and action\n",
    "                state = state_next\n",
    "                action = next_action \n",
    "                step += 1\n",
    "            \n",
    "            self.episode_history[self.episode_number] = episode_data\n",
    "            self.episode_number += 1\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "MiniGrid.train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initialises an empty 20 x 20 grid world environment bounded by walls with a reward in the centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array( # 0 = empty, 1 = wall\n",
    "    [\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ])\n",
    "\n",
    "minigrid = MiniGrid(\n",
    "    grid = grid,\n",
    "    reward_locations = [(9,9),],\n",
    "    reward_values = [10,]\n",
    "    )\n",
    "\n",
    "tdqlearner = TDQLearner(\n",
    "    gamma=0.9, \n",
    "    alpha=0.2, \n",
    "    n_states=minigrid.n_states, \n",
    "    n_actions=minigrid.n_actions)\n",
    "\n",
    "minigrid.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains a `TDQLearner` on the grid world environment for 1000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid.train(\n",
    "    tdqlearner, \n",
    "    n_episodes=1000,\n",
    "    max_episode_length=100,\n",
    "    policy=epsilon_greedy_policy,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this code to plot the training curve and first and last 5 episodes  {display-mode: \"form\" }\n",
    "gridworldtraining_ax = minigrid.plot_training()\n",
    "minigrid.plot_first_and_last_5_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this code to plot the Q values and optimal actions  {display-mode: \"form\" }\n",
    "minigrid.plot_Q(tdqlearner.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final plot displays the Q-values of each state-action pair and, in the centre, the optimal (greedy) policy derived from the Q-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 3.8**\n",
    "> \n",
    "> With this code you should now be able to run quite a few experiments. \n",
    "> 1. üêç **Multiple rewards** Consider a long thin environment (say, 25 wide by 5 deep) with two rewards, one large ($R_1 = 15$ at $(x,y)=(2,2)$) and one small ($R_2 = 5$ at $(x,y)=(22,2)$) a distance $D = 20$ apart. Simulate this and see which reward the optimal agent approaches and from where (let $\\gamma = 1$). \n",
    ">   - üí≠ **[ADVANCED]** If the cost per step is $c = 0.1$ show that the optimal policy is to approach the large reward up to a distance $D_{\\textrm{switch}}$ otherwise take the small reward. Does this match you find in simulation? \n",
    "> $$ D_{\\textrm{switch}} = \\frac{Dc - R_2 + R_1}{2c} = 15$$\n",
    "> 2. üêç Experiment with a different policy generator, for example a stochastic policy generator such as softmax\n",
    "> $$ P(A_t = a | S_t = s) = \\frac{e^{Q(s, a) / \\tau}}{\\sum_{a'} e^{Q(s, a') / \\tau}} $$\n",
    "> 3. **[ADVANCED]** Train the agent on an environment where the reward is hidden behind a wall extending almost (but not quite) across the environment. Can the agent learn to navigate around the wall to the reward?\n",
    ">     - üêç If a small gap in the wall is opened up after learning, test to see if the agent can easily learn to navigate through the gap to the reward.\n",
    ">     - üí≠ TO THINK What are the limitations of tabular Q-learning in this environment regarding generalisation to unseen states?\n",
    "> 4. üí≠ Imagine a much larger 100 x 100 environment: On different trials rewards can appear independently at five locations. Instead of just N, E, S, W the agent can move in any direction in 1 degree increments (i.e. 360 possible actions). Calculate the number of state-action pairs the agent would need to learn to solve this environment.\n",
    ">     - üí≠ TO THINK What are the limitations of tabular Q-learning in this environment regarding scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Multiple rewards simulation  {display-mode: \"form\" }\n",
    "grid = np.array( # 0 = empty, 1 = wall\n",
    "    [\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],        \n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ])\n",
    "\n",
    "minigrid_long = MiniGrid(\n",
    "    grid = grid,\n",
    "    reward_locations = [(2,2),(22,2)],\n",
    "    reward_values = [15,5,],\n",
    "    cost_per_step=1,\n",
    "    )\n",
    "\n",
    "tdqlearner_long = TDQLearner(\n",
    "    gamma=1.0, \n",
    "    alpha=0.1, \n",
    "    n_states=minigrid_long.n_states, \n",
    "    n_actions=minigrid_long.n_actions)\n",
    "\n",
    "minigrid_long.train(tdqlearner_long, \n",
    "               n_episodes=1000, \n",
    "               policy=lambda Q: epsilon_greedy_policy(Q, epsilon=0.1))\n",
    "\n",
    "minigrid_long.plot_training()\n",
    "\n",
    "# Plot Q values along central row and see where west and east cross \n",
    "minigrid_long.plot_policy(tdqlearner_long.Q)   \n",
    "minigrid_long.plot_Q(tdqlearner_long.Q) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution to the hiding-a-reward-behind-the-wall  {display-mode: \"form\" }\n",
    "grid = np.array( # 0 = empty, 1 = wall\n",
    "    [\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ])\n",
    "\n",
    "minigrid_wall = MiniGrid(\n",
    "    grid = grid,\n",
    "    reward_locations = [(14,2),],\n",
    "    reward_values = [10,]\n",
    "    )\n",
    "\n",
    "tdqlearner_wall = TDQLearner(\n",
    "    gamma=0.9, \n",
    "    alpha=0.2, \n",
    "    n_states=minigrid_wall.n_states, \n",
    "    n_actions=minigrid_wall.n_actions)\n",
    "\n",
    "\n",
    "# Train like normal \n",
    "minigrid_wall.train(tdqlearner_wall, \n",
    "               n_episodes=2000, \n",
    "               policy=lambda Q: epsilon_greedy_policy(Q,epsilon=0.1))\n",
    "minigrid_wall.plot_training()\n",
    "ax = minigrid_wall.plot_Q(tdqlearner_wall.Q)\n",
    "ax[0].get_figure().suptitle(\"Policy before a wall is removed\")\n",
    "\n",
    "# Make a gap in the wall and train for another 100 episodes \n",
    "minigrid_wall.grid[15,12] = 0\n",
    "minigrid_wall.render()\n",
    "minigrid_wall.train(tdqlearner_wall, \n",
    "               n_episodes=1000, \n",
    "               policy=lambda Q: epsilon_greedy_policy(Q,epsilon=0.1))\n",
    "minigrid_wall.plot_training()\n",
    "ax = minigrid_wall.plot_Q(tdqlearner_wall.Q)\n",
    "ax[0].get_figure().suptitle(\"Policy after a wall is removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **4. State features and function approximation** <a name=\"dqn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll set ourselves free from the limitation that _every state-action pair_ must be stored in a table. Instead we'll learn a continuous function which maps states to Q-values. \n",
    "\n",
    "We'll call this function the _Q-function_. It will take in the state and returns a vector of Q-values for each action. \n",
    "\n",
    "$$\\hat{Q}_{\\theta}(s,a) $$\n",
    "\n",
    "Where $\\theta$ are the parameters of the function. We'll learn these parameters using the SARSA algorithm._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 State features vs states?**\n",
    "\n",
    "Another change we'll make is that instead of passing the state directly into the Q-function as we did before, we'll pass a _feature vector_ of the state. \n",
    "\n",
    "$$ s \\rightarrow \\mathbf{\\phi}(s) = [\\phi_1(s), \\phi_2(s), \\ldots, \\phi_{N_f}(s)]^{\\top} $$\n",
    "\n",
    "In our case the \"state\" is fully described by the position of the agent in the grid world, so: \n",
    "\n",
    "$$ s \\rightarrow \\mathbf{\\phi}(\\mathbf{x}) = [\\phi_1(\\mathbf{x}), \\phi_2(\\mathbf{x}), \\ldots, \\phi_{N_f}(\\mathbf{x})]^{\\top} $$\n",
    "\n",
    "There are a number of benefits to this:\n",
    "1. **Generalisation**: The function can generalise to unseen states which are similar to states it has seen before. This is because is if $\\mathbf{\\phi}(\\mathbf{x})$ is close to $\\mathbf{\\phi}(\\mathbf{x}^{\\prime})$ then $\\hat{Q}_{\\theta}(\\mathbf{x})$ will be close to $\\hat{Q}_{\\theta}(\\mathbf{x}^{\\prime})$. _Before_, is a state was unseen the agent would have no idea what to do.\n",
    "2. **Biological plausibility**: The brain is thought to represent states using feature vectors. Famous examples include: \n",
    "    - **Place cells** in the hippocampus fire when an animal is in a particular location.\n",
    "    - **Grid cells** in the entorhinal cortex fire in a hexagonal grid pattern across the environment.\n",
    "    - **Boundary Vector Cells** in thesubiculum fire when an animal is near a boundary / wall. \n",
    "    - **Head direction cells** in the thalamus fire when an animal is facing a particular direction.\n",
    "3. **Dimensionality reduction**: The feature vector can be much smaller than the state space. For example, in a grid world with 1000000 \"states\", a well-designed feature vector might only have have 100 features.\n",
    "4. **Feature engineering**: We can design the feature vector to encode, _a priori_, domain knowledge about the environment. For example: \n",
    "    - In a RL task where the inputs are words, we should encode the words as word embeddings (aka. feature vectors). It would be sensible to encode similar words as similar vectors for example \"cat\" and \"feline\" should be close in the feature space. Anything we then learn about \"cat\" should quickly generalise to \"feline\".\n",
    "    - In the grid world we might use states features which are continuous in space (e.g. smooth place cells). That way if we learn that position $(x,y)$ is good we can quickly generalise to $(x+\\delta_x, y+\\delta_y)$.\n",
    "\n",
    "<center><img src=\"./figures/neuro_statefeatures.png\" width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Exercise 4.1**\n",
    ">\n",
    "> 1. üí≠ TO THINK Think of some more _inductive biases_ it might be useful to useful to build into the feature vectors for a spatial navigation task. \n",
    "\n",
    "\n",
    "> üìù **Exercise 4.2 [ADVANCED]**\n",
    ">\n",
    "> 1. üí≠ Starting from the loss function $L_t(\\theta) = (Q_{\\pi}(s,a) - \\hat{Q}(s, a; \\theta))^2$ show that the bootstrapped update rule for $\\theta$ is $\\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_{\\theta} \\hat{Q}(s, a; \\theta)$ where $\\delta_t = R_{t+1} + \\gamma \\hat{Q}(S_{t+1}, A_{t+1}; \\theta) - \\hat{Q}(S_t, A_t; \\theta)$ is the bootstrapped TD-error.\n",
    "> 2. üí≠ Hence show for a linear Q-function $\\hat{Q}(s, a; \\theta) = \\theta^{\\top} \\mathbf{\\phi}(s)$ the optimal update rule is $\\theta \\leftarrow \\theta + \\alpha \\delta_t \\mathbf{\\phi}(s)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Continuous positions _but_ discrete actions**\n",
    "Position is now a continuous variable. So instead of `minigrid.agent_pos = (3,4)` (\"the agent is 3 grids from the left, 4 from the bottom\") its now `minigrid.agent_pos = (0.223,0.186)` (\"the agent is 0.223 m from the left and 0.186 m from the bottom\"). \n",
    "\n",
    "Actions are still discrete but now we'll also allow diagonal movements too (for no particular reason).  \n",
    "- `0` = \"North\", `1` = \"North-East\", `2` = \"East\", `3` = \"South-East\", `4` = \"South\", `5` = \"South-West\", `6` = \"West\", `7` = \"North-West\".\n",
    "\n",
    "> üìù **Exercise 4.3**\n",
    ">\n",
    "> 1. üí≠ TO THINK Do you think it would be possible to have a _continuous_ action space? _Hint: think about any difficulties you might encounter calculating the greedy action._\n",
    "\n",
    "### **4.3 Changes to the python code**\n",
    "\n",
    "There are two key changes which need to be made: \n",
    "1. The `environment` is no longer a `MiniGrid` (discretised) but a `MiniSpace` (continuous). This has already been written for you, the API is almost identical to that of `MiniGrid`.\n",
    "2. The TD learner class _[TO BE WRITTEN BY  YOU]_ has a `self.Q(state, action)` function (rather than a `self.Q[state, action]` table) which takes a state and action as input and returns the Q-value.\n",
    "\n",
    "We'll be using the `RatInABox` (George et al. 2024, https://github.com/RatInABox-Lab/RatInABox) package to handle creating continuous environments and generating biological state features. The following code shows how to make an `Environment`, in which there will be an `Agent` who state is encoded through the firing rate of some `state_features` (any `RatInABox.Neurons` object). Then all three of these are passed into the `MiniSpace` which handles the RL side of things. \n",
    "\n",
    "```python \n",
    "minispace = MiniSpace(\n",
    "    env = env, \n",
    "    ag = ag, \n",
    "    state_features = state_features, \n",
    "    reward_locations=[(0.5,0.3)], \n",
    "    reward_values=[1000,], \n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RatInABox modules\n",
    "from ratinabox.Environment import Environment \n",
    "from ratinabox.Agent import Agent \n",
    "from ratinabox.Neurons import PlaceCells, GridCells, BoundaryVectorCells, HeadDirectionCells\n",
    "\n",
    "# Make a 2D environment, the default size is 1m x 1m\n",
    "env = Environment(params={'dimensionality':'2D','dx':0.02})\n",
    "# Add a wall \n",
    "env.add_wall([[0.2,0.5],[0.8,0.5]])\n",
    "# Add an Agent who position in the environment determines the \"state\"\n",
    "ag = Agent(env)\n",
    "# Make \"state features\" which are the place cells, grid cells etc. \n",
    "state_features_placecells = PlaceCells(ag, params={'n':100, 'widths':0.08})\n",
    "state_features_gridcells = GridCells(ag, params={'n':100, 'gridscale_distribution':'uniform', 'gridscale':(0.1,0.4)})\n",
    "state_features_bvcs = BoundaryVectorCells(ag, params={'n':100})\n",
    "\n",
    "# Plot \n",
    "state_features_placecells.plot_rate_map(chosen_neurons='8')\n",
    "state_features_gridcells.plot_rate_map(chosen_neurons='8')\n",
    "state_features_bvcs.plot_rate_map(chosen_neurons='8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4 Linear Q-function**\n",
    "\n",
    "We'll start by learning a linear Q-function. This is a simple function which takes the dot product of the feature vector and a weight vector.\n",
    "\n",
    "$$ \\hat{Q}_{\\mathsf{W}}(s, a) = [\\mathsf{W}^{\\top} \\mathbf{\\phi}(s)]_{a} $$\n",
    "\n",
    "Here $W \\in \\mathbb{R}_{N_f \\times N_a}$ is the weight matrix and $[\\cdot]_a$ denotes the $a$-th element of the vector.\n",
    "\n",
    "> üìù **Exercise 4.4**\n",
    ">\n",
    "> 1. üêç Complete the `Q` function in the `LinearTDQLearner` class below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTDQLearner():\n",
    "    def __init__(self, gamma=0.5, alpha=0.1, n_features=10, n_actions=8):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.W = np.zeros((n_features, n_actions))\n",
    "\n",
    "    def Q(self, state, action=None):\n",
    "        \"\"\"\n",
    "        This function should return the Q value for a given state and action\n",
    "        State should be a vector of features. Optionally it can be a batch of states where the batch dimension is the first dimension.\n",
    "        If action is None then the function should return the Q values for all actions in the state.\n",
    "        \"\"\"\n",
    "        # append one to the state to account for the bias term\n",
    "        Q_values = np.dot(state, self.W)\n",
    "        return Q_values[...,action] if action is not None else Q_values\n",
    "    \n",
    "    def learn(self, S, S_next, A, A_next, R):\n",
    "        # Get's the value of the current and next state\n",
    "        Q = self.Q(S,A) if S is not None else 0\n",
    "        Q_next = self.Q(S_next, A_next) if S_next is not None else 0\n",
    "        \n",
    "        # Gradient of the Q value with respect to the weights\n",
    "        dQdW = None # ???\n",
    "        # Calculates the TD error (hint remember to use self.gamma\n",
    "        TD_error = None# ???\n",
    "\n",
    "        # Updates the value of the current state\n",
    "        if S is not None:\n",
    "            self.W[:,A] = self.W[:,A] + self.alpha * TD_error * dQdW \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "class LinearTDQLearner():\n",
    "    def __init__(self, gamma=0.5, alpha=0.1, n_features=10, n_actions=8):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.W = np.zeros((n_features, n_actions))\n",
    "\n",
    "    def Q(self, state, action=None):\n",
    "        \"\"\"\n",
    "        This function should return the Q value for a given state and action\n",
    "        State should be a vector of features. Optionally it can be a batch of states where the batch dimension is the first dimension.\n",
    "        If action is None then the function should return the Q values for all actions in the state.\n",
    "        \"\"\"\n",
    "        # append one to the state to account for the bias term\n",
    "        Q_values = np.dot(state, self.W)\n",
    "        return Q_values[...,action] if action is not None else Q_values\n",
    "    \n",
    "    def learn(self, S, S_next, A, A_next, R):\n",
    "        # Get's the value of the current and next state\n",
    "        Q = self.Q(S,A) if S is not None else 0\n",
    "        Q_next = self.Q(S_next, A_next) if S_next is not None else 0\n",
    "        \n",
    "        # Gradient of the Q value with respect to the weights\n",
    "        dQdW = S# ???\n",
    "        # Calculates the TD error (hint remember to use self.gamma\n",
    "        TD_error = R + self.gamma * Q_next - Q\n",
    "\n",
    "        # Updates the value of the current state\n",
    "        if S is not None:\n",
    "            self.W[:,A] = self.W[:,A] + self.alpha * TD_error * dQdW - 0.01 * self.alpha * self.W[:,A] # L2 regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the `MiniSpace`. For the `state_features` we'll be using the place cells we created a few boxes earlier but later you can experiment with others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Make the MiniSpace\n",
    "minispace = MiniSpace(\n",
    "    env = env, \n",
    "    ag = ag, \n",
    "    state_features = state_features_placecells, \n",
    "    reward_locations=[(0.5,0.3)], \n",
    "    reward_values=[10,], \n",
    "    )\n",
    "\n",
    "# Step 3: Make the TDQlearner\n",
    "lineartdqlearner = LinearTDQLearner(\n",
    "    gamma=0.95, \n",
    "    alpha=0.1, \n",
    "    n_features=state_features_placecells.n,\n",
    "    n_actions=minispace.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minispace.train(lineartdqlearner,\n",
    "            n_episodes=1000,\n",
    "            policy=lambda Q: epsilon_greedy_policy(Q,epsilon=0.0))\n",
    "minispace.plot_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minispace.plot_Q(lineartdqlearner.Q)\n",
    "minispace.plot_first_and_last_5_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anim = minispace.animate_episode(episodes=np.arange(minispace.episode_number-1)[-20:], Q_function = lineartdqlearner.Q)\n",
    "# HTML(anim.to_jshtml())\n",
    "# ratinabox.figure_directory = \"./figures\"\n",
    "# ratinabox.utils.save_animation(anim, './rl_animation', anim_save_types=['gif']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **5. Solutions** <a name=\"solutions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to exercise 2.4:\n",
    "\n",
    "The terminal state $S_t = N-1$ has a known value of $V(S = N-1) = \\mathbb{E}[ R(S = N-1)] = 1$ (guaranteed reward of 1). \n",
    "\n",
    "Using the Bellman equation: \n",
    "\n",
    "\\begin{align}\n",
    "V(S_t = n) &= \\mathbb{E} [R_t + \\gamma V(S_{t+1})] \\\\\n",
    "           &= \\mathbb{E} [R_t]  + \\mathbb{E} [\\gamma V(S_{t+1})]  \\\\\n",
    "           &= \\frac{n + 1}{N}\\cdot 1 + \\gamma \\mathbb{E}_{S_{t+1}}[ V(S_{t+1}) ] \\\\\n",
    "           &= \\frac{n + 1}{N}\\cdot 1 + \\underbrace{\\gamma p_t \\cdot V(S_{t+1} = n+1)}_{\\textrm{it transitioned to next state}} + \\underbrace{\\gamma (1-p_t) \\cdot V(S_{t}=n)}_{\\textrm{it stayed in the same state}} \\\\\n",
    "(1 - \\gamma (1-p_t)) V(S_t = n) &= \\frac{n + 1}{N} + \\gamma p_t \\cdot V(S_{t+1} = n+1) \\\\\n",
    "V(S_t = n) &= \\frac{1}{1 - \\gamma (1-p_t)} \\left( \\frac{n + 1}{N} + \\gamma p_t \\cdot V(S_{t+1} = n+1) \\right) \\\\\n",
    "V(n) &= \\frac{1}{1 - \\gamma (1-p_t)} \\left( \\frac{n + 1}{N} + \\gamma p_t \\cdot V(n+1) \\right)\n",
    "\\end{align}\n",
    "  has value $V = R$, state 8 has value $V = 0 + \\gamma * R = \\gamma R$, state 7 has value $V = 0 + \\gamma^2 R$, etc. So state 0 has value $V = \\gamma^9 R$. So if $R=1$ and $\\gamma = 0.9$ then $V = 0.9^9 = 0.387420489$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to exercise 2.5:\n",
    "1. Terminal state 9 has value $V = R$, state 8 has value $V = 0 + \\gamma R = \\gamma R$, state 7 has value $V = 0 + \\gamma^2 R$, etc. So state 0 has value $V = \\gamma^9 R$. So if $R=1$ and $\\gamma = 0.9$ then $V = 0.9^9 = 0.38742$.\n",
    "2. Suppose $\\alpha = 1$ and $\\gamma$ is close to one. The first time the agent receives the reward at state 9 it's value will be updated to $V = 1$ and no further learning will occur on this state (its TD error will be zero). On the next trial the value of state 8 will be updated due to the a TD error because the new value of upcoming state 9 wasn't predicted. Thus, the bump moves backwards at approximately a rate of one-step-each-episode. This makes because each state bootstraps from the next state's value. If if there is 10 steps between state 0 and state 9 then it will take at least 10 episodes for the value of state 0 to be updated and more to converge (depending on the learning rate and other factors).\n",
    "3. The residual TD-error at the start is because the first state is never predictable. Pavlov's dog may be able to associate the bell with the food, but it can't predict the bell so hearing the bell will always come as a positive surprise (aka. a positive TD-error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to exercise 3.1: \n",
    "\n",
    "**Question 1:**\n",
    "\n",
    "Recall the Bellman equation for taking action $A_t$ in state $S_t$ and transitioning to state $S_{t+1}$ getting reward $R_{t+1}$: $Q_{\\pi}(S_t, A_t) = \\mathbb{E} \\big[ R_{t+1} + \\gamma Q_{\\pi}(S_{t+1}, \\pi(S_{t+1})  \\big]$. In our case everything is deterministic so we can drop the expectation.\n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\pi_1}(S_2, A_2) &= 1 + \\gamma Q_{\\pi_1}(S_2, \\pi_1(S_2)) \\\\\n",
    "                    &= 1 + \\gamma Q_{\\pi_1}(S_2,A_2) \\\\\n",
    "                    &= \\frac{1}{1 - \\gamma} \n",
    "\\end{align}\n",
    "\n",
    "Likewise \n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\pi_1}(S_1, A_1) &= 2 + \\gamma Q_{\\pi_1}(S_2, \\pi_1(S_2)) \\\\\n",
    "                    &= 2 + \\gamma Q_{\\pi_1}(S_2,A_2) \\\\\n",
    "                    &= 2 + \\gamma \\frac{1}{1 - \\gamma} \\\\\n",
    "                    &= \\frac{2 - \\gamma}{1 - \\gamma}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\pi_1}(S_1, A_2) &= 1 + \\gamma Q_{\\pi_1}(S_1, \\pi_1(S_1)) \\\\\n",
    "                    &= 1 + \\gamma Q_{\\pi_1}(S_1,A_1) \\\\\n",
    "                    &= 1 + \\gamma \\frac{2 - \\gamma}{1 - \\gamma} \\\\\n",
    "                    &= \\frac{1 + \\gamma - \\gamma^2}{1 - \\gamma}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\pi_1}(S_2, A_1) &= 3 + \\gamma Q_{\\pi_1}(S_1, \\pi_1(S_1)) \\\\\n",
    "                    &= 3 + \\gamma Q_{\\pi_1}(S_1,A_1) \\\\\n",
    "                    &= 3 + \\gamma \\frac{2 - \\gamma}{1 - \\gamma} \\\\\n",
    "                    &= \\frac{3 - \\gamma - \\gamma^2}{1 - \\gamma}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "**Question 2:**\n",
    "The optimal policy, $\\pi^{*}$ is to take action $A_1$ in state $S_1$ and action $A_1$ in state $S_2$. \n",
    "\n",
    "**Question 3:**\n",
    "The value of each state-action pair under the optimal policy $\\pi^{*}$ is:\n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\pi^{*}}(S_1, A_1) &= 2 + \\gamma Q_{\\pi^{*}}(S_2, \\pi^{*}(S_2)) \\\\\n",
    "                    &= 2 + \\gamma Q_{\\pi^{*}}(S_2,A_1) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\pi^{*}}(S_2, A_1) &= 3 + \\gamma Q_{\\pi^{*}}(S_1, \\pi^{*}(S_1)) \\\\\n",
    "                    &= 3 + \\gamma Q_{\\pi^{*}}(S_1,A_1) \\\\\n",
    "\\end{align}\n",
    "Solving these simultaneously gives:\n",
    "\\begin{align}\n",
    "Q_{\\pi^{*}}(S_2, A_1) &= \\frac{3 + 2\\gamma}{1 - \\gamma^2} \\\\\n",
    "Q_{\\pi^{*}}(S_1, A_1) &= \\frac{2 + 3\\gamma}{1 - \\gamma^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "For the other state-action pairs:\n",
    "\\begin{align}\n",
    "Q_{\\pi^{*}}(S_1, A_2) &= 1 + \\gamma Q_{\\pi^{*}}(S_1, \\pi^{*}(S_1)) \\\\\n",
    "                    &= 1 + \\gamma Q_{\\pi^{*}}(S_1,A_1) \\\\\n",
    "                    &= 1 + \\gamma \\frac{2 + 3\\gamma}{1 - \\gamma^2} \\\\\n",
    "                    &= \\frac{1 + 2\\gamma + 2\\gamma^2}{1 - \\gamma^2} \\\\\n",
    "Q_{\\pi^{*}}(S_2, A_2) &= 1 + \\gamma Q_{\\pi^{*}}(S_2, \\pi^{*}(S_2)) \\\\\n",
    "                    &= 1 + \\gamma Q_{\\pi^{*}}(S_2,A_1) \\\\\n",
    "                    &= 1 + \\gamma \\frac{3 + 2\\gamma}{1 - \\gamma^2} \\\\\n",
    "                    &= \\frac{1 + 3\\gamma + \\gamma^2}{1 - \\gamma^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to exercise 3.5:\n",
    "\n",
    "1. A good example might be moving to a new neighbourhood which you don't know well. You must decide whether to exploit what you already know (e.g. go to the same restaurant you always go to) or explore new options (try a new restaurant, which, given your lack of knowledge, might be better or worse than your usual choice). Another example might be the Netflix recommendation algorithm which must balance showing you things you already like (exploitation) with showing you new things you might like (exploration).\n",
    "2. In a stable environment that is well-understood and not subject to change, it is probably better to prioritize exploitation. An example of such an environment is a manufacturing assembly line with consistent demand for a specific product. In this case, it is best to exploit what you know works to maximize efficiency and output, rather than exploring new methods that may be less efficient or break the system.\n",
    "3. In an unstable and constantly changing environment that is not well understood, it is better to prioritize exploration. An example of such an environment is the early stages of a startup in a rapidly evolving technology sector. In this case, it is important to explore new ideas and methods to adapt to the changing landscape and find the best path forward. Another example might be starting a new job: it's worth exploring different ways of working, different projects and different collaborators to find the best fit.\n",
    "4. Many real world scenarios _stochastic_ policies are optimal. One example is bluffing in poker. If you always  bluff when you have a poor hand your opponents will quickly learn this and exploit you by betting against you. If you never bluff then your opponents will always fold when you have a good hand and you will never win much money. However, if you bluff randomly (stochastically) then your opponents will be unable to predict your behaviour and will be forced to play more cautiously. Rock-Paper-Scissor is a similar. Another example is the behaviour of animals in the wild: if a predator always follows the same path it will be easy for its prey to avoid it. However, if it follows a stochastic path then it will be more likely to catch its prey. \n",
    "5. Softmax action selection is a popular alternative to $\\epsilon$-greedy. In softmax action selection, the probability of selecting an action is proportional to the exponentiated value of the Q-value for that action. This allows for a smooth transition between exploration and exploitation, with the probability of selecting the best action increasing as the Q-values become more certain.\n",
    "$$ P(A_t = a | S_t = s) = \\frac{e^{Q(s, a) / \\tau}}{\\sum_{a'} e^{Q(s, a') / \\tau}} $$\n",
    "where $\\tau$ is a temperature parameter that controls the degree of exploration. When $\\tau$ is high, the policy is close to uniform random action selection, and as $\\tau$ approaches zero, the policy becomes deterministic and selects the action with the highest Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurorl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
