{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neuro RLs** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TomGeorge1234/NeuroRLTutorial/blob/main/NeuroRL.ipynb)\n",
    "### **University of Amsterdam Neuro-AI Summer School ,2024**\n",
    "#### made by: **Tom George (UCL) and Jesse Geerts (Imperial)**\n",
    "\n",
    "In this tutorial we'll study and build reinforcement learning models inspired by the brain. \n",
    "\n",
    "TODO: Insert fun animation of the model we'll end up building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Contents** \n",
    "0. [Import dependencies and data](#dependencies)\n",
    "1. [Rescorla-Wagner Model](#rescorla)\n",
    "2. [Temporal Difference Learning](#td)\n",
    "3. [Q-Learning](#q)\n",
    "    1. [Navigating in a grid world](#grid)\n",
    "4. [Deep Q-Learning](#dqn)\n",
    "    1. [Neuroscience inspired basis functions](#basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **0. Import dependencies and data** <a name=\"dependencies\"></a>\n",
    "Run the following code: It'll install some dependencies, download some files and import some functions. You can mostly ignore it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see code {display-mode: \"form\" }\n",
    "\n",
    "!pip install wget ratinabox \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import wget \n",
    "from IPython.display import HTML\n",
    "#if running on colab we need to download the data and utils files\n",
    "if os.path.exists(\"NeuroRL_utils.py\"):\n",
    "    print(\"utils located\")\n",
    "    pass\n",
    "else: \n",
    "    wget.download(\"https://github.com/TomGeorge1234/NeuroRLTutorial/raw/main/NeuroRL_utils.py\")\n",
    "    print(\"...utils downloaded!\")\n",
    "\n",
    "from NeuroRL_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **1. Rescorla-Wagner** <a name=\"rescorla\"></a>\n",
    "\n",
    "Classical conditioning is where a neutral stimulus -- also called the _unconditioned stimulus, US_ -- is paired with a response-producing stimulus -- _the conditioned stimulus, CS_. After the association is learned, the neutral stimulus *alone* can produce the response.\n",
    "\n",
    "The most famous example is Pavlov's dogs: Pavlov rang a bell before feeding his dogs which would cause them to salivate. After a while, the dogs would start salivating when they heard the bell, even if no food was presented.\n",
    "\n",
    "In 1972 Rescorla and Wagner proposed a simple model to explain this learning process. The model is based on the idea that the strength of the association between the CS and US is proportional to the discrepancy between the expected and actual US.\n",
    "\n",
    "### **1.1. Model**\n",
    "Following on from the Pavlov's dogs example, let's say the bell is the conditioned stimulus, $CS$, and the food is the unconditioned stimulus, $US$ which comes with a response (reward) of strength $R$. The _value_ of the stimulus, $\\hat{V}$, is the strength of the association between the $i^{th}$ stimulus and the unconditioned response.\n",
    "\n",
    "Mathematically, the model is defined as follows:\n",
    "\n",
    "$$ \\hat{V} \\leftarrow \\hat{V} + \\alpha \\cdot \\underbrace{(R - \\hat{V})}_{\\delta = \\textrm{``error\"}}$$\n",
    "\n",
    "I.e. the increment in the value of the stimulus is proportional to the discrepancy between the reward (the unconditioned response) that was recieved, $R$, and the value of the stimulus, $\\hat{V}$. The proportionality constant $\\alpha$ is the learning rate.\n",
    "\n",
    "**Exercise 1.1** \n",
    "1. Consider a simple example where there is only one stimulus with zero initial value. A constant reward, $R$ is given each trial. Show the value of the stimulus after the first trial is given by $V(1) = \\alpha  \\cdot R$.\n",
    "2. Show that $\\hat{V}(t) = R \\cdot (1 - e^{-\\alpha\\cdot t})$  (_Hint: consider using the change of of variables $U(t) = R - \\hat{V}(t)$._)\n",
    "\n",
    "### **1.2 Model implementation**\n",
    "\n",
    "Below we provide some basic code implementing a Rescorla Wagner model. \n",
    "\n",
    "**Exercise 1.2**\n",
    "1. Complete the `def learn(self, R, alpha):` function to implement the Rescorla-Wagner learning rule. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagner(BaseRescorlaWagner):\n",
    "    def __init__(self, alpha = 0.1,): \n",
    "        \"\"\"alpha is the learning rate\"\"\"\n",
    "        self.V = 0 # initial value\n",
    "        self.R_history = [None] # list to store the reward recieved on each trial\n",
    "        self.V_history = [self.V] # list to store the predicted value on each trial\n",
    "        super().__init__()\n",
    "\n",
    "    def learn(self, R, alpha=0.1):\n",
    "        \"\"\"R is the reward recieved on the current trial, alpha is the learning rate\"\"\"\n",
    "        raise NotImplementedError(\"You need to implement this method\")\n",
    "        # error = ???\n",
    "        # self.V += ???\n",
    "        # self.R_history.append(R)\n",
    "        # self.V_history.append(V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, R, alpha=0.1):\n",
    "    \"\"\"R is the reward recieved on the current trial, alpha is the learning rate\"\"\"\n",
    "    error = R - self.V\n",
    "    self.V += alpha * error\n",
    "    self.R_history.append(R)\n",
    "    self.V_history.append(self.V) \n",
    "RescorlaWagner.learn = learn # set the learn method to the function we just defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run an experiment where a reward of 1 is given each trial. We'll plot the value of the stimulus over time using the pre-written `RescorlaWagner.plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your learning rate and reward\n",
    "alpha = 0.1\n",
    "R = 1\n",
    "\n",
    "RW = RescorlaWagner()\n",
    "for trial in range(100):\n",
    "    RW.learn(R=R, alpha=alpha)\n",
    "ax = RW.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3**\n",
    "\n",
    "Plot the theoretical solution you derived earlier onto the `ax` and see if it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Click to see solution {display-mode: \"form\" }\n",
    "t_range = np.arange(100)\n",
    "R = 1\n",
    "alpha = 0.1\n",
    "V = R * (1 - np.exp(-t_range*alpha))\n",
    "ax.plot(t_range, V, label='Analytic solution', linewidth=0.5, color='k', linestyle='--')\n",
    "ax.legend()\n",
    "ax.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.4**\n",
    "\n",
    "1. **Acquisition** Repeat the above experiment with a lower and a higher learning rate. What do you observe?\n",
    "2. **Extinction** Repeat the above but this time reward is given only for the first 50 trials, then the reward is set to zero. What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "R = 1\n",
    "RW = RescorlaWagner()\n",
    "for trial in range(100):\n",
    "    RW.learn(R=1, alpha=alpha)\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Higher learning rate\")\n",
    "\n",
    "# Set your learning rate and reward\n",
    "alpha = 0.05\n",
    "R = 1\n",
    "RW = RescorlaWagner()\n",
    "for trial in range(100):\n",
    "    RW.learn(R=1, alpha=alpha)\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Lower learning rate\")\n",
    "\n",
    "\n",
    "# Set your learning rate and reward\n",
    "alpha = 0.1\n",
    "R = 1\n",
    "RW = RescorlaWagner()\n",
    "for trial in range(50):\n",
    "    RW.learn(R=1, alpha=alpha)\n",
    "for trial in range(50):\n",
    "    RW.learn(R=0, alpha=alpha)\n",
    "ax = RW.plot()\n",
    "ax.set_title(\"Extinction experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3. Rescorla-Wagner with multiple unconditioned stimuli**\n",
    "\n",
    "It's easy to extend the Rescorla-Wagner model to multiple stimuli:\n",
    "\n",
    "* Multiple stimuli are represented by a vectors, e.g.:  \n",
    "    * Stimulus A: $S = [1, 0]$ means there are 2 stimuli, only the first of which was present on this particular trial. \n",
    "    * Stimulus B: $S = [0, 1]$ means only the second stimulus was present.\n",
    "    * Stimulus A & B: $S = [1, 1]$ means both stimuli were present.\n",
    "* A vector of association \"weights\", $W$, denotes the strength of the association between each stimulus and the unconditioned response (i.e. the value of each stimulus). \n",
    "    * $W = [W_1, W_2]$.\n",
    "* The total value of the stimuli is the sum of the values of each stimulus present on a given trial: \n",
    "$$ \\hat{V} = S \\cdot W $$\n",
    "\n",
    "The full Rescorla-Wagner model is then:\n",
    "$$\\vec{W} = \\vec{W} + \\alpha \\big(R - \\hat{V}\\big) \\vec{S}$$\n",
    "\n",
    "**Exercise 1.5**\n",
    "1. Reason why $\\vec{S}$ now appears in the learning rule.\n",
    "\n",
    "**Exercise 1.6**\n",
    "As before, complete the `def learn(self, R, S, alpha):` function to implement the Rescorla-Wagner learning rule for multiple stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagner_multistim(BaseRescorlaWagner):\n",
    "    def __init__(self, n_stimuli=2): \n",
    "        \"\"\"alpha is the learning rate\"\"\"\n",
    "        self.W = np.zeros(n_stimuli)\n",
    "        self.W_history = np.array([self.W])\n",
    "        self.R_history = [None]\n",
    "        self.V_history = [0]\n",
    "        super().__init__(n_stimuli=n_stimuli)\n",
    "\n",
    "    def learn(self, S, R, alpha=0.05):\n",
    "        \"\"\"S is the stimulus vector. R is the reward recieved on the current trial\"\"\"\n",
    "        V = S @ self.W\n",
    "        error = R - V\n",
    "        self.W += alpha * S * error\n",
    "\n",
    "        # store the history\n",
    "        self.W_history = np.vstack([self.W_history, self.W])\n",
    "        self.R_history.append(R)\n",
    "        self.V_history.append(S @ self.W)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.7**:\n",
    "1. With your partner implement these four experiments in the Rescorla-Wagner model:\n",
    "    1. **Blocking**\n",
    "        * A single stimulus is paired with the US (A --> R), then a compound stimulus is paired with the US (AB --> R). What happens?\n",
    "    2. **Overshadowing**\n",
    "        * Two stimuli are paired with a reward (AB --> R) but one is much more salient than the other, e.g. $\\vec{S} = [1, 0.1]$.\n",
    "    3. **Overexpectation**\n",
    "        * Two stimuli are seperately paired with the US (A --> R, B --> R), then the compound stimulus is presented (AB --> ?). What happens?\n",
    "    4. **Conditioned Inhibition**\n",
    "        * A single stimulus is paired with the US (A --> R) then a second stimulus is added and the reward is removed. (AB --> _). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the blocking experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_multistim(n_stimuli=2)\n",
    "for i in range(100):\n",
    "    RW.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(100):\n",
    "    RW.learn(S=np.array([1, 1]), R=1)\n",
    "RW.plot()\n",
    "print(RW.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the overshadowing experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_MultipleStimuli(n_stimuli=2)\n",
    "for i in range(100):\n",
    "    RW.learn(S=np.array([0.9, 0.1]), R=1)\n",
    "RW.plot()\n",
    "print(RW.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the overexpectation experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_MultipleStimuli(n_stimuli=2)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([0, 1]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 1]), R=1)\n",
    "RW.plot()\n",
    "print(RW.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the inhibition experiment goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double click to see solution {display-mode: \"form\" }\n",
    "RW = RescorlaWagner_MultipleStimuli(n_stimuli=2)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 0]), R=1)\n",
    "for i in range(50):\n",
    "    RW.learn(S=np.array([1, 1]), R=0)\n",
    "ax = RW.plot()\n",
    "#put legend below plot \n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Temporal difference learning** <a name=\"td\"></a>\n",
    "\n",
    "One limitation of the Rescorla-Wagner model is that it doesn't take into account the temporal structure of the environment. Associations are made between stimuli _now_ and rewards _now_. In reality, rewards are often delayed.\n",
    "\n",
    "Temporal difference (TD) learning takes into account the temporal structure of the environment. As you learnt in today's lecture the idea is that \"value\" of a state is based not only on the reward recieved _now_ but on all the rewards that _will be_ recieved in the future.\n",
    "\n",
    "$$V(S_t) = \\mathbb{E} \\big[ \\underbrace{R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots}_{G_t = \\textrm{``return'' from state $_{S_t}$}}\\big]$$\n",
    "\n",
    "To formalise notation, we'll define $\\hat{V}(S_t)$ as _our current estimate of the value of a state_ $V(S_t)$. The goal of learning is to to refine $\\hat{V}(S_t)$ to be as close as possible to $V(S_t)$.\n",
    "\n",
    "**Exercise 2.1**\n",
    "By considering the following loss function: \n",
    "$$L_t = \\big[ \\hat{V}(S_t) - V(S_t) \\big]^2$$\n",
    "show, by gradient descent, that the optimal update rule for $\\hat{V}(S_t)$ is:\n",
    "$$\\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha \\big[ V(S_t) - \\hat{V}(S_t) \\big]$$\n",
    "\n",
    "### **Monte-Carlo learning**\n",
    "One way to estimate the value of a state is to wait until the end of each episode, collecting all the rewards that were recieved along the way and calculate the single-episode return $G_t$. Since the _expectation_ of $G_t$ is equal to $V(S_t)$ this is equivalent to _stochastic gradient descent_ of the loss function and is called Monte-Carlo learning.\n",
    "\n",
    "$$ \\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha  \\big[ G_t - \\hat{V}(S_t) \\big] $$\n",
    "\n",
    "Although in theory this does work, in practice, Monte-Carlo learning is often infeasible because it requires waiting until the end of each episode to update the value of each state. This turns out to be quite a serious limitation in real-world applications - imagine waiting until the end of a game of chess to update the value of each state! Or worse, some environments don't have a clearly defined end such as the game of life we're all currently playing. This is where TD learning comes in.\n",
    "\n",
    "\n",
    "### **TD-Learning**\n",
    "The key idea behind TD-learning is to estimate the value of a state by bootstrapping from the value of the next state. \n",
    "\n",
    "**Exercise 2.2**\n",
    "1. Show that the value of a state can be written as the sum of the reward recieved at that state and the value of the next state. i.e.\n",
    "$$V(S_t) = \\mathbb{E} [R_t + \\gamma V(S_{t+1})]. $$\n",
    "\n",
    "This is called the _Bellman equation_ and is the basis of TD-learning. Its encodes a _very_ important idea: \n",
    "\n",
    "**Bellman Equation:  The future value of a state now is equal to the reward recieved now plus the value of the next state (discounted a little bit).**\n",
    "\n",
    "But wait! How do we know the value of the next state? We don't! That's why we're learning! So we'll use our current estimate of the value of the next state, $\\hat{V}(S_{t+1})$ as a _proxy_.\n",
    "\n",
    "$$V(S_t) = \\mathbb{E} [\\color{red}{\\underbrace{R_t + \\gamma V(S_{t+1}}_{\\textrm{I don't know this}}} \\color{d}{}] \\approx  \\mathbb{E}[\\color{green}{\\underbrace{R_t + \\gamma \\hat{V}(S_{t+1})}_{\\textrm{I do know this}}}\\color{d}{}] $$\n",
    "\n",
    "This gives us the TD-learning update rule:\n",
    "\n",
    "$$\\hat{V}(S_t) \\leftarrow \\hat{V}(S_t) + \\alpha \\big[\\underbrace{R_t + \\gamma \\hat{V}(S_{t+1}) - \\hat{V}(S_t)}_{\\delta_t = \\textrm{``TD-error''}} \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**\n",
    "1. Implement the TD-learning update rule in the `def learn(self, R, S, S_next, alpha):` function in the `TDLearner` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLearner(BaseTDLearner):\n",
    "    def __init__(self, gamma=0.5, alpha=0.1, n_states=10):\n",
    "        \"\"\"TD learner class. \n",
    "        Args:\n",
    "            gamma (float): discount factor\n",
    "            alpha (float): learning rate\n",
    "            n_states (int): number of distinct states in the environment\"\"\"\n",
    "        super().__init__(gamma=gamma, alpha=alpha, n_states=n_states)\n",
    "\n",
    "    def perform_episode(self, \n",
    "                        states : np.ndarray, \n",
    "                        rewards : np.ndarray,):\n",
    "        \n",
    "        assert len(states) == len(rewards), \"states and rewards must have the same length\"\n",
    "        T_episode = len(states)\n",
    "        TD_error = np.empty(len(states))\n",
    "        States = np.empty((T_episode, self.n_states))\n",
    "        TD = np.empty(T_episode)\n",
    "        for i in range(len(states)):\n",
    "            S = states[i]\n",
    "            S_next = states[i+1] if i+1 < len(states) else None\n",
    "            R = rewards[i]\n",
    "            self.learn(S, S_next, R)\n",
    "            TD_error[i] = self.TD_error\n",
    "        \n",
    "            state_as_onehot = np.zeros(self.n_states)\n",
    "            state_as_onehot[S] = 1\n",
    "            States[i,:] = state_as_onehot\n",
    "            self.R_history.append(rewards)\n",
    "            TD[i] = self.TD_error\n",
    "        self.V_history.append(self.V.copy())\n",
    "        self.S_history.append(States)\n",
    "        self.TD_history.append(TD)\n",
    "\n",
    "        \n",
    "    def learn(self, S, S_next, R):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "        S is the state index, an integer between 0 and n_states-1 specifying the current state.\n",
    "        S_next is the state index, an integer between 0 and n_states-1 specifying the next state.\n",
    "        R is the reward recieved on the current trial\n",
    "        alpha is the learning rate\"\"\"\n",
    "        V = self.V[S]\n",
    "        V_next = self.V[S_next] if S_next is not None else 0\n",
    "        # perform the TD update for the _previous_ state\n",
    "        self.TD_error = R + self.gamma * V_next - V\n",
    "        self.V[S] = self.V[S] + self.alpha * self.TD_error\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters \n",
    "n_episodes = 50\n",
    "gamma = 0.95\n",
    "alpha = 0.5\n",
    "n_states = 31\n",
    "\n",
    "# Initialize the TD learner\n",
    "TD = TDLearner(gamma=gamma, n_states=n_states)\n",
    "\n",
    "# Run the experiment\n",
    "for episode in range(n_episodes):\n",
    "    states = np.arange(n_states)\n",
    "    rewards = np.array([0]*(n_states-1) + [1]) # only reward on the last state\n",
    "    TD.perform_episode(states=states, rewards=rewards)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "TD.plot(episode=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = TD.animate_plot()\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurorl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
