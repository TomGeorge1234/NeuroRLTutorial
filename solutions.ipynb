{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Solutions to the Neuro RL tutorial exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.11 \n",
    "1. Would _not_ be captured by the current Rescorla Wagner model\n",
    "2. Would _not_ be captured by the current Rescorla Wagner model\n",
    "3. Would be captured by the current Rescorla Wagner model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "1. \n",
    "\n",
    "2. From state $S_0$ you recieve rewards of $1, 1, 1, \\ldots$ on to infinity. The value of the state $S_0$ is $1 + \\gamma\\cdot 1 + \\gamma^2\\cdot 1 + \\ldots$ which is a geometric series that converges to $\\frac{1}{1 - \\gamma}$. By symmetry, the value of the state $S_1$ is $\\frac{1}{1 - \\gamma}$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "1. Some features you may build into your feature vecotr are: \n",
    "    1. Proximity to boundaries or obstacles (better decision making regarding the environment) \n",
    "    2. Orientation of the agent. \n",
    "    3. Velocity and acceleration of the agent (help in understanding how motion and dynamics of the agent affect future states)\n",
    "    4. Previous states or memory (in complex worlds the Markov assumption may break down and its useful to allow previou sstates to influence future actions) \n",
    "    5. Energy constraint of the agent (if the agent has limited energy, it may need to conserve energy and make decisions accordingly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurorl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
